{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7537330f",
   "metadata": {},
   "source": [
    "# Start Ollama Server\n",
    "\n",
    "1. Open a terminal\n",
    "\n",
    "    ☰ > *Terminal* > *New Terminal*\n",
    "\n",
    "2. Start the Ollama server \n",
    "\n",
    "    ```bash\n",
    "    ollama serve\n",
    "    ```\n",
    "\n",
    "3. Do not close the terminal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485eea1",
   "metadata": {},
   "source": [
    "# Define Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93995f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff3793",
   "metadata": {},
   "source": [
    "# Initialize Ollama Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267b6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# minimize randomness for reproducibility\n",
    "seed = 42\n",
    "temperature = 0\n",
    "\n",
    "# Set up the Ollama chat model with specified parameters\n",
    "llm = ChatOllama(\n",
    "    base_url=os.environ[\"OLLAMA_BASE_URL\"],\n",
    "    model=\"mistral\",\n",
    "    temperature=temperature,\n",
    "    seed=seed,\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cf451",
   "metadata": {},
   "source": [
    "# Test Ollama Chatbot\n",
    "\n",
    "1. Run the cell to start a conversation with the chatbot. Type `stop` to exit at any time.\n",
    "2. Modify its behavior by changing the system instructions (e.g., \"You are a mean assistant. Answer my questions in a mean way.\").\n",
    "3. Interact with the chatbot again to observe the changes.\n",
    "4. If you discover something interesting or unexpected, feel free to share it with the person next to you or even the group!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0eb4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "write a poem\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      " In the quiet of the morning, as the sun begins to rise,\n",
      "    A gentle whisper of the day, beneath the vast, clear skies.\n",
      "    The world awakes with a soft, sweet sigh,\n",
      "    As dew-kissed petals open wide.\n",
      "\n",
      "    The rustling leaves, they dance and sway,\n",
      "    In the breeze that carries secrets away.\n",
      "    The chirping birds, their melodious song,\n",
      "    A symphony of life, where all belong.\n",
      "\n",
      "    The river flows with a steady grace,\n",
      "    Reflecting the world in its tranquil space.\n",
      "    Mountains stand tall, ancient and wise,\n",
      "    Guardians of secrets hidden in the skies.\n",
      "\n",
      "    In the heart of the city, amidst the hustle and bustle,\n",
      "    Lives a quiet peace, a gentle ruffle.\n",
      "    A moment's pause, a breath taken deep,\n",
      "    A reminder to cherish life's simple sweep.\n",
      "\n",
      "    So let us live with hearts open wide,\n",
      "    Embrace each day, our spirits tied.\n",
      "    For in the grandeur of this world so vast,\n",
      "    We find our home, where we are enchanted and cast.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "exit\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      " I'm sorry for any confusion, but as a text-based AI model, I don't have the ability to exit or close down. I'm here to assist you with your questions and help generate text based on your prompts. If you have any other questions or need further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Set up chat instructions and history\n",
    "instructions = \"You are an helpful assistant. Answer my questions in a helpful way.\"\n",
    "history = [SystemMessage(content=instructions)]\n",
    "\n",
    "while True: \n",
    "    prompt = input(\"Prompt (Enter 'stop' to exit)\")\n",
    "    if prompt == \"stop\": \n",
    "        break\n",
    "    # Add user message to history\n",
    "    history.append(HumanMessage(content=prompt))\n",
    "    # Display user message\n",
    "    history[-1].pretty_print()\n",
    "    # Prepare and display AI message header\n",
    "    ai_message = AIMessage(content=\"\")\n",
    "    ai_message.pretty_print()\n",
    "    print()\n",
    "    # Stream and display AI response\n",
    "    for chunk in llm.stream(history):\n",
    "        print(chunk.content, end=\"\")\n",
    "        ai_message.content += chunk.content\n",
    "    print()\n",
    "    # Add AI response to history\n",
    "    history.append(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0108ec",
   "metadata": {},
   "source": [
    "# Limited Capabilities of LLM-based Chatbots\n",
    "\n",
    "LLM-based chatbots often struggle with certain types of problems, such as arithmetic tasks. See the demo below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96cbffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are an helpful assistant. Answer my questions in a helpful way.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How many 1 in 111111111111111?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      " The number 1 appears exactly once in the sequence 111111111111111, as you specified. It is located at the first position (index 0).\n",
      "RIGHT ANSWER: 15 times\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Create a chat history with a system message and a human message\n",
    "history = [\n",
    "    SystemMessage(content=\"You are an helpful assistant. Answer my questions in a helpful way.\"),\n",
    "    HumanMessage(content=\"How many 1 in 111111111111111?\"), # 15 ones\n",
    "]\n",
    "# Invoke the model with the chat history\n",
    "ai_message = llm.invoke(history)\n",
    "history.append(ai_message)\n",
    "\n",
    "# display the chat history\n",
    "for message in history:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0ecc3",
   "metadata": {},
   "source": [
    "<div style= \"padding: 0.5em;background-color: rgba(255,0,0, 0.5);width: 95%\">\n",
    "\n",
    "### Why does the LLM fail to solve the counting task in the demo above?\n",
    "\n",
    "LLMs process text as a continuous stream of **tokens**—units of text such as whole words or word fragments—each assigned a unique numerical identifier. While they excel at generating coherent and contextually relevant responses, LLMs are not designed for precise calculations. They lack an inherent ability to accurately count specific characters, words, or phrases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005cd615",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
