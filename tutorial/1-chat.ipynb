{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5661053",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 0, 0, 0.5); padding: 10px; border-radius: 5px; width: 95%\">\n",
    "\n",
    "### üîí **Running a Large Language Model (LLM) locally**\n",
    "\n",
    "Large Language Models (LLMs) have become increasingly popular for various applications, including natural language processing, text generation, and more. However, they are hosted on cloud platforms, which can be costly and may raise privacy concerns. Running an LLM locally allows you to have more control over your data and potentially reduce costs.\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0.5); padding: 10px; border-radius: 5px; width: 95%\">\n",
    "\n",
    "### üöÄ **Purpose of This Notebook**\n",
    "\n",
    "This notebook demonstrates how to pull and serve a Large Language Model (LLM) locally using `Ollama` and to interact with it via  `LangChain` Python libary. \n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537330f",
   "metadata": {},
   "source": [
    "# üöÄ Start Ollama Server\n",
    "\n",
    "Follow these steps to launch and prepare the Ollama server:\n",
    "\n",
    "1. **Open a terminal**  \n",
    "   ‚ò∞ ‚Üí *Terminal* ‚Üí *New Terminal*\n",
    "\n",
    "2. **Start the Ollama server** by running:\n",
    "\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "\n",
    "3. **Do not close this terminal.**\n",
    "   Keep it running in the background.\n",
    "\n",
    "4. **Open another terminal**\n",
    "\n",
    "5. **Pull the model** `qwen3:4b` *(if not already done)*:\n",
    "\n",
    "   ```bash\n",
    "   ollama pull qwen3:4b\n",
    "   ```\n",
    "\n",
    "6. **Close this second terminal**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485eea1",
   "metadata": {},
   "source": [
    "# üîí Define Global Constants\n",
    "\n",
    "Set up global constants to use throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93995f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "LLM_MODEL = \"qwen3:4b\"\n",
    "LLM_SEED = 42 \n",
    "LLM_TEMPERATURE = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff3793",
   "metadata": {},
   "source": [
    "# ü§ñ Initialize Ollama Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267b6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# minimize randomness for reproducibility\n",
    "seed = 42\n",
    "temperature = 0\n",
    "\n",
    "# Set up the Ollama chat model with specified LLM model and parameters\n",
    "llm = ChatOllama(\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    model=LLM_MODEL,\n",
    "    temperature=LLM_TEMPERATURE,\n",
    "    seed=LLM_SEED,\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cf451",
   "metadata": {},
   "source": [
    "# üí¨ Chat with the Assistant\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0); padding: 10px; border: 1px solid rgba(255, 255, 255, 1); border-radius: 5px; width: 95%; margin: 10px auto; text-align: center;\">\n",
    "\n",
    "### üìä Conversation Flow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "   State0((START))\n",
    "   State1(Instructions)\n",
    "   State2(Assistant)\n",
    "   State3(Human)\n",
    "   State5((END))\n",
    "   State0 --> State1\n",
    "   State1 -- SystemMessage --> State2\n",
    "   State2 -- AiMessage --> State3\n",
    "   State3 -- HumanMessage --> State2\n",
    "   State3 -- type `stop` --> State5\n",
    "```\n",
    "</div>\n",
    "\n",
    "\n",
    "Follow the steps below to start and customize your conversation:\n",
    "\n",
    "1. **Run the cell** to start chatting with the assistant.  \n",
    "   ‚§∑ Type `stop` at any time to exit.\n",
    "\n",
    "2. **Customize the assistant's behavior** by editing the system instructions.  \n",
    "   Example:  \n",
    "   ```python\n",
    "   instructions = \"You are a mean assistant. Answer my questions in a mean way.\"\n",
    "   ```\n",
    "\n",
    "<!-- State4(All messages are added to the conversation history before being sent to the model) -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant. Answer my questions in a helpful way.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Initialize conversation history\n",
    "history = []\n",
    "# Prepare instructions for the assistant\n",
    "instructions = \"You are a helpful assistant. Answer my questions in a helpful way.\"\n",
    "# Create a system message with the instructions\n",
    "system_message = SystemMessage(content=instructions)\n",
    "# Add the system message to the conversation history\n",
    "history.append(system_message)\n",
    "\n",
    "while True: \n",
    "    # Clear the output and display the conversation history\n",
    "    clear_output(wait=True)\n",
    "    for message in history:\n",
    "        message.pretty_print()\n",
    "        time.sleep(0.1)  # Add a slight delay for better readability\n",
    "    # Get user prompt\n",
    "    prompt = input(\"Prompt (Enter 'stop' to exit)\")\n",
    "    if prompt == \"stop\": \n",
    "        break\n",
    "    # Create and display a human message with the user prompt\n",
    "    human_message = HumanMessage(content=prompt)\n",
    "    human_message.pretty_print()\n",
    "    # Add the human message to history\n",
    "    history.append(human_message)\n",
    "    # Create an AI message to hold the assistant response\n",
    "    ai_message = AIMessage(content=\"\")\n",
    "    # Display the empty AI message\n",
    "    ai_message.pretty_print()\n",
    "    print()\n",
    "    # Stream the assistant response and update the AI message content\n",
    "    for chunk in llm.stream(history):\n",
    "        print(chunk.content, end=\"\")\n",
    "        ai_message.content += chunk.content\n",
    "    print()\n",
    "    # Sanitize the AI message content\n",
    "    ai_message.content = re.sub(r\"<think>.*?</think>\", \"\", ai_message.content, flags=re.DOTALL).strip()\n",
    "    # Add the AI message to history\n",
    "    history.append(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da17787",
   "metadata": {},
   "source": [
    "# üß± Chatbot Class\n",
    "\n",
    "To simplify usage, we've encapsulated the chatbot logic into a Python class.\n",
    "\n",
    "üìÅ See the implementation in: `src/chatbot.py`\n",
    "\n",
    "This class wraps the setup and interaction steps you‚Äôve seen earlier, making it easier to use the chatbot across multiple notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Chatbot import Chatbot\n",
    "# Create a Chatbot instance with the Ollama LLM\n",
    "chatbot = Chatbot(llm=llm, history=history)\n",
    "# Interact with the chatbot\n",
    "chatbot.interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a72ee",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "üí° **Starting in the next Python notebooks**, we'll use this class to interact with the chatbot in a more structured and reusable way.\n",
    "\n",
    "\n",
    "If you are interested in chating with the assistant using a user-friendly interface, you can use one of the following options:\n",
    "\n",
    "- [Msty](https://msty.app/)\n",
    "- [Open Web UI](https://docs.openwebui.com/)\n",
    "- [LM Studio](https://lmstudio.ai/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
