{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860bdd6d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 0, 0, 0.5); padding: 10px; border-radius: 5px; width: 95%\">\n",
    "\n",
    "### **Limited autonomy of Large Language Models (LLMs)**\n",
    "\n",
    "Large Language Models (LLMs) do not have real-time awareness or access to current events. Their knowledge is based on static training data with a fixed cut-off date.\n",
    "They also cannot browse the web, run code, or interact with live tools unless explicitly connected to such systems.\n",
    "\n",
    "As a result, when asked questions that require up-to-date information or external actions, LLMs may:\n",
    "\n",
    "- Provide outdated, inaccurate, or nonsensical answers, **or**\n",
    "- Admit they don’t know the answer.\n",
    "\n",
    "**Example:**  \n",
    "-  Ask the chatbot: _\"What time is it?\"_  \n",
    "- and observe how it responds.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42159320",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.5); padding: 10px; border-radius: 5px; width: 95%\">\n",
    "\n",
    "### **Purpose of the Notebook**\n",
    "The goal of the notebook is to demonstrate how to overcome the limited autonomy of LLMs by connecting them to external tools and APIs, enabling them to access real-time information and perform actions beyond their static training data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eefbf0",
   "metadata": {},
   "source": [
    "# Start Ollama Server\n",
    "\n",
    "1. Open a terminal\n",
    "\n",
    "    ☰ > *Terminal* > *New Terminal*\n",
    "\n",
    "2. Start the Ollama server \n",
    "\n",
    "    ```bash\n",
    "    ollama serve\n",
    "    ```\n",
    "\n",
    "3. Do not close this terminal\n",
    "\n",
    "4. Open another terminal\n",
    "5. Pull the model *\"qwen3:4b\"* (if not already done)\n",
    "\n",
    "    ```bash\n",
    "    ollama pull qwen3:4b\n",
    "    ```\n",
    "5. Close this terminal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485eea1",
   "metadata": {},
   "source": [
    "# Define Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93995f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "LLM_MODEL = \"qwen3:4b\"\n",
    "LLM_SEED = 42 \n",
    "LLM_TEMPERATURE = 0.0\n",
    "TEST_PROMPT0 = \"What time is it?\"\n",
    "TEST_PROMPT1 = \"What is the price of gold right now?\" \n",
    "TEST_PROMPT3 = \"How many 1 in 111111111111111?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff3793",
   "metadata": {},
   "source": [
    "# Initialize Ollama Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267b6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Set up the Ollama chat model with specified LLM model and parameters\n",
    "llm = ChatOllama(\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    model=LLM_MODEL,\n",
    "    temperature=LLM_TEMPERATURE,\n",
    "    seed=LLM_SEED,\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cb34e",
   "metadata": {},
   "source": [
    "# Define Tools\n",
    "\n",
    "1. **Python Tool:** This tool allows the chatbot to execute Python code, enabling it to perform calculations, data processing, and other tasks that require programming logic.\n",
    "\n",
    "2. **Commodity Price Tool:** This tool fetches real-time commodity prices from the `API Ninjas` API, allowing the chatbot to provide up-to-date information on various commodities.\n",
    "    - Go to [``API Ninjas``](https://api-ninjas.com/)\n",
    "    - Click the \"Sign Up\" button\n",
    "    - Create an account\n",
    "    - Log in\n",
    "    - Click the \"My Account\" button\n",
    "    - Click the \"Show API Key\" button\n",
    "\n",
    "    In the next cell, you will need to:\n",
    "    - Replace the content of the variable `NINJA_API_KEY` with your API key.\n",
    "    - Run the cell to test the API tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88126c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "{\"exchange\": \"CME\", \"name\": \"Gold Futures\", \"price\": 3368, \"updated\": 1749218320}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool, Tool\n",
    "from typing import Annotated, List\n",
    "import io\n",
    "import contextlib\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "NINJA_API_KEY = \"MdaCcVUHseixkwpNGhumWg==9mb676ZThl4du9aP\"\n",
    "\n",
    "@tool\n",
    "def execute_python(py_code: Annotated[str, \"Python code to execute\"]) -> str:\n",
    "    \"\"\"Executes a Python code and returns its standard output (you have to use the print() function).\"\"\"\n",
    "    output = io.StringIO()\n",
    "    try:\n",
    "        with contextlib.redirect_stdout(output):\n",
    "            exec(py_code, {})\n",
    "        return output.getvalue().strip() or \"Code executed with no output.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test the tool\n",
    "print(execute_python.invoke(\"print('Hello, World!')\"))\n",
    "\n",
    "@tool\n",
    "def get_commodity_price(\n",
    "    commodity: Annotated[str, \"The name of the commodity to get the price for\"]\n",
    ") -> str:\n",
    "    \"\"\"Get the current price of a commodity using the Ninja API.\"\"\"\n",
    "    api_url = 'https://api.api-ninjas.com/v1/commodityprice?name={}'.format(commodity)\n",
    "    response = requests.get(api_url, headers={'X-Api-Key': NINJA_API_KEY})\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        return response.text\n",
    "    else:\n",
    "        return f\"Error {response.status_code}: {response.text}\"\n",
    "\n",
    "#Test the tool\n",
    "print(get_commodity_price.invoke(\"gold\"))\n",
    "    \n",
    "tools = [execute_python, get_commodity_price]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ad674",
   "metadata": {},
   "source": [
    "# Prepare Reason-and-Act (ReAct) Instructions\n",
    "\n",
    "ReAct (Reason-and-Act) prompting combines:\n",
    "\n",
    "- **Reasoning:** The model explains its thinking process.\n",
    "- **Acting:** The model chooses and performs actions (like using a calculator, web search, or database query).\n",
    "\n",
    "The prompt encourages the model to alternate between these two steps, creating a loop:\n",
    "\n",
    "→ Think → Act → Observe → Think → Act → … until the task is complete.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "(Yao et al., 2023) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2023, January). ReAct: Synergizing reasoning and acting in language models. In *International Conference on Learning Representations (ICLR)*.]{https://doi.org/10.48550/arXiv.2210.03629}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f08829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can.\n",
      "You can answer directly if the user is greeting you or similar.\n",
      "Otherwise, you have access to the following tools:\n",
      "\n",
      "execute_python(py_code: Annotated[str, 'Python code to execute']) -> str - Executes a Python code and returns its standard output (you have to use the print() function).\n",
      "get_commodity_price(commodity: Annotated[str, 'The name of the commodity to get the price for']) -> str - Get the current price of a commodity using the Ninja API.\n",
      "\n",
      "The way you use the tools is by specifying a json blob.\n",
      "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
      "\n",
      "The only values that should be in the \"action\" field are: execute_python, get_commodity_price\n",
      "\n",
      "The $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"action\": $TOOL_NAME,\n",
      "  \"action_input\": $INPUT\n",
      "}\n",
      "```\n",
      "\n",
      "ALWAYS use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action:\n",
      "```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Reminder to always use the exact characters `Final Answer:` when responding.\n",
      "Always make your final answer easy to read and understand for humans.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from inspect import signature\n",
    "\n",
    "tools_names = \", \".join([tool.name for tool in tools])\n",
    "tools_descriptions = \"\\n\".join([f\"{tool.name}{signature(tool.func)} - {tool.description}\" for tool in tools]) \n",
    "\n",
    "\n",
    "# Taken from https://smith.langchain.com/hub/hwchase17/react-json\n",
    "instructions = f\"\"\"Answer the following questions as best you can.\n",
    "You can answer directly if the user is greeting you or similar.\n",
    "Otherwise, you have access to the following tools:\n",
    "\n",
    "{tools_descriptions}\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
    "\n",
    "The only values that should be in the \"action\" field are: {tools_names}\n",
    "\n",
    "The $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n",
    "\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "ALWAYS use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Reminder to always use the exact characters `Final Answer:` when responding.\n",
    "Always make your final answer easy to read and understand for humans.\n",
    "\"\"\"\n",
    "\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab489e",
   "metadata": {},
   "source": [
    "# Create a Chatbot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "960f5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import BaseMessage, AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "class Chatbot:\n",
    "    llm: ChatOllama\n",
    "    history: List[BaseMessage]\n",
    "\n",
    "    def __init__(self, llm: ChatOllama, history: List[BaseMessage] = []):\n",
    "        \"\"\"Initialize the chatbot with an LLM and an optional history.\"\"\"\n",
    "        self.llm = llm\n",
    "        self.history = history\n",
    "    \n",
    "    def invoke(self, prompt:str) -> None:\n",
    "        \"\"\"Run the chatbot with the current history.\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        self.pretty_print()\n",
    "        human_message = HumanMessage(content=prompt)\n",
    "        human_message.pretty_print()\n",
    "        self.history.append(human_message)\n",
    "        ai_message = AIMessage(content=\"\")\n",
    "        ai_message.pretty_print()\n",
    "        print()\n",
    "        for chunk in self.llm.stream(self.history):\n",
    "            print(chunk.content, end=\"\")\n",
    "            ai_message.content += chunk.content\n",
    "        print()\n",
    "        self.history.append(ai_message)\n",
    "        \n",
    "    \n",
    "    def interact(self) -> None:\n",
    "        \"\"\"Run the chatbot in interactive mode.\"\"\"\n",
    "        while True: \n",
    "            prompt = input(\"Prompt (Enter 'stop' to exit)\")\n",
    "            if prompt == \"stop\": \n",
    "                break\n",
    "            self.invoke(prompt)\n",
    "\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        \"\"\"Pretty print the chatbot's history.\"\"\"\n",
    "        for message in self.history:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be529b36",
   "metadata": {},
   "source": [
    "# Test the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef58eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat history with a system message and a human message\n",
    "history = [SystemMessage(content=instructions)]\n",
    "chatbot = Chatbot(llm=llm, history=history)\n",
    "# chatbot.invoke(TEST_PROMPT0)\n",
    "# chatbot.interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee0b09",
   "metadata": {},
   "source": [
    "<div style= \"padding: 0.5em;background-color: rgba(255,0,0, 0.5);width: 95%\">\n",
    "\n",
    "### **Problem:** the LLM generated the observation and should have waited for the tool response\n",
    "\n",
    "Here, we did not interrupt the LLM when it generated the keyword *\"Observation:\"*. Meaning that we did not actually call the tool. Hence, the LLM hallucinated and made up a response.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bfd24",
   "metadata": {},
   "source": [
    "# Interrupting the chatbot when generating '*Observations:*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a8e7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "binded_llm = llm.bind(stop=[\"Observation:\"])\n",
    "history = [SystemMessage(content=instructions)]\n",
    "chatbot = Chatbot(llm=binded_llm, history=history)\n",
    "#chatbot.invoke(TEST_PROMPT0)\n",
    "#chatbot.interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1b9e4",
   "metadata": {},
   "source": [
    "# Create an Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea10267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseMessage, AIMessage, HumanMessage, SystemMessage\n",
    "import re\n",
    "from typing import Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish, AgentActionMessageLog\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain.agents.agent import AgentOutputParser\n",
    "from langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\n",
    "import ast\n",
    "from IPython.display import clear_output, display\n",
    "from langchain_core.messages import ToolMessage\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "class Agent(Chatbot):\n",
    "\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def __init__(self, llm: ChatOllama, tools: List[Tool], history: List[BaseMessage] = []):\n",
    "        \"\"\"Initialize the chatbot with an LLM, tools, and an optional history.\"\"\"\n",
    "        super().__init__(llm, history)\n",
    "        self.tools = tools\n",
    "\n",
    "    def invoke(self, prompt:str) -> None:\n",
    "        \"\"\"Run the chatbot in interactive mode.\"\"\"\n",
    "        self.history.append(HumanMessage(content=prompt))\n",
    "        clear_output(wait=True)\n",
    "        self.pretty_print()\n",
    "        stop = False\n",
    "        while not stop:\n",
    "            ai_pre_action_message = self.llm.invoke(self.history)\n",
    "            self.history.append(ai_pre_action_message)\n",
    "            try:\n",
    "                action = self.parse_action(ai_pre_action_message.content)\n",
    "                if isinstance(action, AgentAction):\n",
    "                    tool_message = self.call_tool(action)\n",
    "                    self.history.append(tool_message)\n",
    "                if isinstance(action, AgentFinish):\n",
    "                    stop = True\n",
    "            except SyntaxError as e:\n",
    "                self.history.append(SystemMessage(content=str(e)))\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                sys.exit()\n",
    "            clear_output(wait=True)\n",
    "            self.pretty_print()\n",
    "\n",
    "\n",
    "    def call_tool(self, action: AgentAction) -> ToolMessage:\n",
    "        \"\"\"Call the specified tool with the given action input.\"\"\"\n",
    "        tool = next((t for t in self.tools if t.name == action.tool), None)\n",
    "        if not tool:\n",
    "            return ToolMessage(content=f\"Error: Tool '{action.tool}' does not exist.\", tool_call_id=\"unknown_tool\")\n",
    "        result = None\n",
    "        sig = signature(tool.func)\n",
    "        if len(sig.parameters):\n",
    "            action_input = action.tool_input\n",
    "            if not isinstance(action_input, dict):\n",
    "                param_name = next(iter(sig.parameters))\n",
    "                action_input = {param_name: action_input}\n",
    "            result = tool.func(**action_input)\n",
    "        else:\n",
    "            result = tool.func()\n",
    "        return ToolMessage(content=f\"Observation: {result}\", tool_call_id=tool.func.__name__)\n",
    "    \n",
    "    \n",
    "    def parse_action(self, text:str) -> Union[AgentAction, AgentActionMessageLog, AgentFinish]:\n",
    "        \"\"\"Parse the action from the LLM output text and return an AgentAction or AgentFinish object.\"\"\"\n",
    "        FINAL_ANSWER_ACTION = \"Final Answer:\"\n",
    "        pattern = re.compile(r\"^.*?`{3}(?:json)?\\n?(.*?)`{3}.*?$\", re.DOTALL)\n",
    "        includes_answer = FINAL_ANSWER_ACTION in text\n",
    "        try:\n",
    "            found = pattern.search(text)\n",
    "            if not found:\n",
    "                raise ValueError(\"action not found.\")\n",
    "            action = found.group(1)\n",
    "            response = ast.literal_eval(action.strip())\n",
    "            return AgentAction(response[\"action\"], response.get(\"action_input\", {}), text)\n",
    "        except Exception as e:\n",
    "            if not includes_answer:\n",
    "                raise SyntaxError(\"Reminder to always use the exact characters `Final Answer:` when responding.\")\n",
    "            output = text.split(FINAL_ANSWER_ACTION)[-1].strip()\n",
    "            return AgentFinish({\"output\": output}, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e0dea",
   "metadata": {},
   "source": [
    "# Testing the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d90d63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Answer the following questions as best you can.\n",
      "You can answer directly if the user is greeting you or similar.\n",
      "Otherwise, you have access to the following tools:\n",
      "\n",
      "execute_python(py_code: Annotated[str, 'Python code to execute']) -> str - Executes a Python code and returns its standard output (you have to use the print() function).\n",
      "get_commodity_price(commodity: Annotated[str, 'The name of the commodity to get the price for']) -> str - Get the current price of a commodity using the Ninja API.\n",
      "\n",
      "The way you use the tools is by specifying a json blob.\n",
      "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
      "\n",
      "The only values that should be in the \"action\" field are: execute_python, get_commodity_price\n",
      "\n",
      "The $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"action\": $TOOL_NAME,\n",
      "  \"action_input\": $INPUT\n",
      "}\n",
      "```\n",
      "\n",
      "ALWAYS use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action:\n",
      "```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Reminder to always use the exact characters `Final Answer:` when responding.\n",
      "Always make your final answer easy to read and understand for humans.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What time is it?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking, \"What time is it?\" Hmm, I need to figure out how to respond. Let me check the tools available. The tools provided are execute_python and get_commodity_price. The execute_python tool allows me to run Python code and get the output. The get_commodity_price is for getting prices of commodities.\n",
      "\n",
      "Wait, the user is asking for the current time. The tools I have don't include a clock or time function. But maybe I can use the execute_python tool to run a Python script that gets the current time. Let me think. In Python, I can use the datetime module to get the current time. So I can write a Python code snippet that prints the current time. Then, the execute_python tool would run that code and return the output. That should give me the current time. Let me try that.\n",
      "</think>\n",
      "\n",
      "Thought: I need to determine the current time. I can use the execute_python tool to run a Python script that gets the current time.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"execute_python\",\n",
      "  \"action_input\": \"from datetime import datetime\\nprint(datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\"))\"\n",
      "}\n",
      "```\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Observation: 2025-06-06 14:12:19\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user asked for the current time, and I used the execute_python tool to run a script that prints the current time. The output from the tool is \"2025-06-06 14:12:19\". That's the current date and time in the format year-month-day hour:minute:second.\n",
      "\n",
      "So, the current time is June 6, 2ight, 14:12:19. I should present this information clearly to the user. Since the user might not need the detailed format, I can simplify it to just the time part, \"14:12:19\", but maybe include the date as well for context. Alternatively, I can state it as \"June 6, 2025 at 2:12:19 PM\" to make it more readable. However, since the tool's output is in 24-hour format, it's safer to present it as is unless the user specifies a different format. \n",
      "\n",
      "I should check if the user needs the time in a specific format, but since they didn't mention it, the output from the tool is acceptable. Therefore, the final answer is the current time as provided by the tool.\n",
      "</think>\n",
      "\n",
      "Final Answer: The current time is June 6, 2025 at 2:12:19 PM.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the price of gold right now?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking for the current price of gold. Let me check the tools I have available. There's the get_commodity_price function which can be used to fetch the price of a commodity, including gold. I should call that function with \"gold\" as the commodity parameter. I need to make sure the action is correctly formatted in JSON. Let me structure the action properly.\n",
      "</think>\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_commodity_price\",\n",
      "  \"action_input\": \"gold\"\n",
      "}\n",
      "```\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Observation: {\"exchange\": \"CME\", \"name\": \"Gold Futures\", \"price\": 3361.9, \"updated\": 1749218552}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user asked for the current price of gold. I used the get_commodity_price tool with \"gold\" as the input. The response came back with a price of 3361.9. Now I need to present this information clearly. The observation includes details like exchange, name, and updated timestamp, but the main price is 3361.9. I should format the answer to show the price in a readable way, maybe including that it's for Gold Futures on CME. Also, the updated time is 1749218552, which I can convert to a more readable format. Let me check the current time based on that timestamp. Wait, the timestamp is in seconds since epoch. Let me calculate the date. But maybe the user just needs the price. Alternatively, since the previous answer for the time was in 2025, maybe this is a test scenario. Anyway, the main point is to report the price as per the tool's response. So the final answer should state the price of gold is $3,361.90 per ounce, based on the CME Gold Futures data. I should also mention that it's the latest updated price. Let me make sure the numbers are correctly formatted with commas and decimal points.\n",
      "</think>\n",
      "\n",
      "Final Answer: The current price of gold (Gold Futures) is $3,361.90 per ounce, as of the latest update via CME.\n"
     ]
    }
   ],
   "source": [
    "# Create a chat history with a system message and a human message\n",
    "binded_llm = llm.bind(stop=[\"Observation:\"])\n",
    "history = [SystemMessage(content=instructions)]\n",
    "agent = Agent(llm=binded_llm, tools=tools, history=history)\n",
    "agent.invoke(TEST_PROMPT0)\n",
    "agent.invoke(TEST_PROMPT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be09a7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
