{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8939324c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(255, 0, 0, 0.5); padding: 10px; border-radius: 5px; width: 95%\">\n",
    "\n",
    "### ðŸ”’ **Mastering Prompting Techniques for Better LLM Results**\n",
    "\n",
    "Large Language Models (LLMs) are powerful tools for solving a wide range of problems. However, the quality of their output depends heavily on how you prompt them. By using effective prompting techniques, you can unlock more advanced capabilities, improve accuracy, and make LLMs more reliable for your specific tasks.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0.5); padding: 10px; border-radius: 5px; width: 95%\">\n",
    "\n",
    "### ðŸš€ **Purpose of This Notebook**\n",
    "\n",
    "This notebook introduces practical prompting strategiesâ€”such as few-shot prompting, chain-of-thought reasoning, and othersâ€”to guide LLMs toward producing high-quality outputs. You'll learn how to run a Large Language Model locally using `Ollama`, and how to implement these prompting methods with the `LangChain` Python library.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537330f",
   "metadata": {},
   "source": [
    "# ðŸš€ Start Ollama Server\n",
    "\n",
    "Follow these steps to launch and prepare the Ollama server:\n",
    "\n",
    "1. **Open a terminal**  \n",
    "   â˜° â†’ *Terminal* â†’ *New Terminal*\n",
    "\n",
    "2. **Start the Ollama server** by running:\n",
    "\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "\n",
    "3. **Do not close this terminal.**\n",
    "   Keep it running in the background.\n",
    "\n",
    "4. **Open another terminal**\n",
    "\n",
    "5. **Pull the model** `qwen3:4b` *(if not already done)*:\n",
    "\n",
    "   ```bash\n",
    "   ollama pull qwen3:4b\n",
    "   ```\n",
    "\n",
    "6. **Close this second terminal**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485eea1",
   "metadata": {},
   "source": [
    "# ðŸ”’ Define Global Constants\n",
    "\n",
    "Set up global constants to use throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93995f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "LLM_MODEL = \"qwen3:4b\"\n",
    "LLM_SEED = 42 \n",
    "LLM_TEMPERATURE = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff3793",
   "metadata": {},
   "source": [
    "# ðŸ¤– Initialize Ollama Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Set up the Ollama chat model with specified LLM model and parameters\n",
    "llm = ChatOllama(\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    model=LLM_MODEL,\n",
    "    temperature=LLM_TEMPERATURE,\n",
    "    seed=LLM_SEED,\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da17787",
   "metadata": {},
   "source": [
    "# ðŸ“¥ Importing the Chatbot Class\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0); padding: 10px; border: 1px solid rgba(255, 255, 255, 1); border-radius: 5px; width: 95%; margin: 10px auto; text-align: center;\">\n",
    "\n",
    "### ðŸ“Š Conversation Flow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "   State0((START))\n",
    "   State1(Instructions)\n",
    "   State2(Assistant)\n",
    "   State3(Human)\n",
    "   State5((END))\n",
    "   State0 --> State1\n",
    "   State1 -- SystemMessage --> State2\n",
    "   State2 -- AiMessage --> State3\n",
    "   State3 -- HumanMessage --> State2\n",
    "   State3 -- type `stop` --> State5\n",
    "```\n",
    "</div>\n",
    "\n",
    "To use the chatbot in your notebooks or scripts, import the `Chatbot` class from the `src` module where it's defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Chatbot import Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c7731",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Zero-shot Prompting\n",
    "\n",
    "Zero-shot prompting leverages the ability of Large Language Models (LLMs) to understand and perform tasks they have **never explicitly seen during training**.\n",
    "\n",
    "For example, you can instruct an LLM to classify text into categories like *positive*, *negative*, or *neutral* â€” a task commonly known in Machine Learning as **sentiment analysis**.\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0); padding: 10px; border: 1px solid rgba(255, 255, 255, 1); border-radius: 5px; width: fit-content; margin: 10px auto;\">\n",
    "ðŸ’¡ Unlike traditional models that require task-specific training data, zero-shot prompting enables the model to generalize based solely on the prompt description.\n",
    "</div>\n",
    "\n",
    "### ðŸ“š Reference\n",
    "\n",
    "[Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., ... & Le, Q. V. (2021). Finetuned language models are zero-shot learners. *arXiv preprint arXiv:2109.01652.*](https://doi.org/10.48550/arXiv.2109.01652)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c42274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "\n",
    "instructions = \"You are a Sentiment analyzer. Classify the human messages into neutral, negative, or positive sentiment. Only return the sentiment classification.\"\n",
    "chatbot = Chatbot(llm=llm, history=[SystemMessage(content=instructions)])\n",
    "\n",
    "chatbot.invoke(\"I love programming in Python!\")  # Example prompt (positive sentiment)\n",
    "chatbot.invoke(\"I hate bugs in my code!\")  # Another example prompt (negative sentiment)\n",
    "chatbot.invoke(\"The sky is blue.\")  # Example prompt (neutral sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e816f",
   "metadata": {},
   "source": [
    "## ðŸ“š Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting leverages the ability of Large Language Models (LLMs) to **learn a task from only a small number of labeled examples** (called *shots*), without needing large datasets.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "- The model is provided with a handful of examples (e.g., 2, 5, or 10) for each class or task.\n",
    "- Using these examples, the model generalizes to classify or predict new, unseen inputs.\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0); padding: 10px; border: 1px solid rgba(255, 255, 255, 1); border-radius: 5px; width: fit-content; margin: 10px auto;\">\n",
    "ðŸ’¡ This approach bridges the gap between zero-shot prompting (no examples) and full supervised learning (large labeled datasets).\n",
    "</div>\n",
    "\n",
    "\n",
    "### ðŸ“š Reference\n",
    "\n",
    "[Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *Advances in neural information processing systems, 33, 1877-1901*.](https://doi.org/10.48550/arXiv.2005.14165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Example 1:  \n",
    "Text: \"Alice visited Paris last summer.\"  \n",
    "Entities: Alice (Person), Paris (Location)\n",
    "\n",
    "Example 2:  \n",
    "Text: \"Google was founded by Larry Page and Sergey Brin.\"  \n",
    "Entities: Google (Organization), Larry Page (Person), Sergey Brin (Person)\n",
    "\n",
    "Example 3:  \n",
    "Text: \"Amazon is headquartered in Seattle.\"  \n",
    "Entities: Amazon (Organization), Seattle (Location)\n",
    "\n",
    "Now identify entities (Organization, Person, location) in the following human messages.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = Chatbot(llm=llm, history=[SystemMessage(content=instructions)])\n",
    "\n",
    "chatbot.invoke(\"Microsoft acquired LinkedIn in 2016.\")  # Example prompt\n",
    "chatbot.invoke(\"Elon Musk is the CEO of SpaceX and Tesla.\")  # Another example prompt\n",
    "chatbot.invoke(\"The Eiffel Tower is located in Paris, France.\")  # Example prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773db1e8",
   "metadata": {},
   "source": [
    "# ðŸ”— Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Chain-of-Thought (CoT) prompting is a technique that guides Large Language Models (LLMs) to **break down complex problems into smaller, logical steps**. By encouraging step-by-step reasoning, CoT helps the model arrive at more accurate and transparent solutions.\n",
    "\n",
    "For example, instead of directly giving the answer to a math problem, the model is prompted to explain the reasoning process step-by-step before concluding.  \n",
    "Example prompt:  \n",
    "`\"Think step by step and provide the final answer at the end.\"`\n",
    "\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0); padding: 10px; border: 1px solid rgba(255, 255, 255, 1); border-radius: 5px; width: fit-content; margin: 10px auto;\">\n",
    "ðŸ’¡ Recent LLMs like GPT-4 and LLaMA 3 now have built-in chain-of-thought capabilities as part of their cold start (e.g., via text between <code>&lt;think&gt;</code> and <code>&lt;/think&gt;</code>).  \n",
    "This means CoT reasoning happens automatically without special prompting, unless you explicitly want the model to show its intermediate steps in the final answer.\n",
    "</div>\n",
    "\n",
    "### ðŸ“š Reference\n",
    "\n",
    "[Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. *Advances in neural information processing systems, 35, 24824-24837*.](https://doi.org/10.48550/arXiv.2201.11903)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"You are a helpful assistant. Answer my questions in a helpful way.\"\"\"\n",
    "prompt = \"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the convenience store owner.\n",
    "Then I went to buy 5 more apples and ate 1. How many apples do I have left?\n",
    "\"\"\"\n",
    "chatbot = Chatbot(llm=llm, history=[SystemMessage(content=instructions)])\n",
    "chatbot.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2deab4a",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  Prompt Chaining\n",
    "\n",
    "Prompt chaining is a technique used with AI language models where you **break down a complex task into a sequence of linked prompts**. Each prompt uses the output from the previous one to build a chain of reasoning or content creation.\n",
    "\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Start with an initial prompt:** Ask the model a simple or first-step question.\n",
    "2. **Use the output as context:** Feed the AIâ€™s response into a new prompt that asks for elaboration or the next step.\n",
    "3. **Repeat as needed:** Continue chaining prompts until the final detailed answer or content is produced.\n",
    "\n",
    "## Examples\n",
    "- **Example 1:** Summarize an article â†’ Critique the summary â†’ Refine based on feedback.\n",
    "- **Example 2:** Generate a code â†’ Identify bugs or inefficiencies â†’ Refactor the code.\n",
    "\n",
    "### ðŸ“š Reference\n",
    "\n",
    "[Wu, T., Terry, M., & Cai, C. J. (2022, April). Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In *Proceedings of the 2022 CHI conference on human factors in computing systems (pp. 1-22)*.](https://doi.org/10.1145/3491102.3517582)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import re\n",
    "\n",
    "# Helper function to clean up AI message content by removing <think> tags\n",
    "def sanitize(message: AIMessage) -> str:\n",
    "    \"\"\"Remove <think> tags and extra whitespace from the message content.\"\"\"\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", message.content, flags=re.DOTALL).strip()\n",
    "\n",
    "# Set up chatbot instructions and initialize\n",
    "chatbot = Chatbot(\n",
    "    llm=llm,\n",
    "    history=[SystemMessage(content=\"You are a helpful assistant. Answer my questions in a helpful way.\")]\n",
    ")\n",
    "\n",
    "# --- PROMPT 0: Summarize a medical research paper ---\n",
    "RESEARCH_PAPER = \"\\n\".join([\n",
    "    page.page_content for page in PyPDFLoader(\"files/daniel2025shypins.pdf\").lazy_load()\n",
    "])\n",
    "prompt0 = f\"\"\"Summarize this research paper.\\n\\n<paper>\\n{RESEARCH_PAPER}\\n</paper>\\n\\nFocus on motivation, methodology, and findings.\\n\"\"\"\n",
    "chatbot.clear_history()\n",
    "SUMMARY = sanitize(chatbot.invoke(prompt0))\n",
    "\n",
    "# --- PROMPT 1: Provide feedback on the summary ---\n",
    "prompt1 = f\"\"\"Your task is to provide feedback on a research paper summary. Here is a summary of a research paper:\\n\\n<summary>\\n{SUMMARY}\\n</summary>\\n\\nHere is the research paper:\\n\\n<paper>\\n{RESEARCH_PAPER}\\n</paper>\\n\\nReview this summary for accuracy, clarity, and completeness on a graded A-F scale.\\n\"\"\"\n",
    "chatbot.clear_history()\n",
    "FEEDBACK = sanitize(chatbot.invoke(prompt1))\n",
    "\n",
    "# --- PROMPT 2: Improve the summary based on feedback ---\n",
    "prompt2 = f\"\"\"Your task is to improve a paper summary given feedback. Here is the first draft of a medical research paper:\\n<summary>\\n{SUMMARY}\\n</summary>\\n\\nHere is the research paper:\\n<paper>\\n{RESEARCH_PAPER}\\n</paper>\\n\\nHere is the feedback:\\n<feedback>\\n{FEEDBACK}\\n</feedback>\\n\\nUpdate the summary based on the feedback.\\n\"\"\"\n",
    "chatbot.clear_history()\n",
    "CORRECTED_SUMMARY = sanitize(chatbot.invoke(prompt2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
