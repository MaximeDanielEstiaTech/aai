{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7537330f",
   "metadata": {},
   "source": [
    "# 🚀 Start Ollama Server\n",
    "\n",
    "Follow these steps to launch and prepare the Ollama server:\n",
    "\n",
    "1. **Open a terminal**  \n",
    "   ☰ → *Terminal* → *New Terminal*\n",
    "\n",
    "2. **Start the Ollama server** by running:\n",
    "\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "\n",
    "3. **Do not close this terminal.**\n",
    "   Keep it running in the background.\n",
    "\n",
    "4. **Open another terminal**\n",
    "\n",
    "5. **Pull the model** `qwen3:4b` *(if not already done)*:\n",
    "\n",
    "   ```bash\n",
    "   ollama pull qwen3:4b\n",
    "   ```\n",
    "\n",
    "6. **Close this second terminal**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485eea1",
   "metadata": {},
   "source": [
    "# 🔒 Define Global Constants\n",
    "\n",
    "Set up global constants to use throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93995f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "LLM_MODEL = \"qwen3:4b\"\n",
    "LLM_SEED = 42 \n",
    "LLM_TEMPERATURE = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff3793",
   "metadata": {},
   "source": [
    "# 🤖 Initialize Ollama Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267b6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Set up the Ollama chat model with specified LLM model and parameters\n",
    "llm = ChatOllama(\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    model=LLM_MODEL,\n",
    "    temperature=LLM_TEMPERATURE,\n",
    "    seed=LLM_SEED,\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da17787",
   "metadata": {},
   "source": [
    "# 📥 Importing the Chatbot Class\n",
    "\n",
    "To use the chatbot in your notebooks or scripts, import the `Chatbot` class from the `src` module where it's defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2598eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Chatbot import Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c7731",
   "metadata": {},
   "source": [
    "# 🎯 Zero-shot Prompting\n",
    "\n",
    "Zero-shot prompting leverages the ability of Large Language Models (LLMs) to understand and perform tasks they have **never explicitly seen during training**.\n",
    "\n",
    "For example, you can instruct an LLM to classify text into categories like *positive*, *negative*, or *neutral* — a task commonly known in Machine Learning as **sentiment analysis**.\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0); padding: 10px; border: 1px solid rgba(255, 255, 255, 1); border-radius: 5px; width: fit-content; margin: 10px auto;\">\n",
    "💡 Unlike traditional models that require task-specific training data, zero-shot prompting enables the model to generalize based solely on the prompt description.\n",
    "</div>\n",
    "\n",
    "### 📚 Reference\n",
    "\n",
    "[Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., ... & Le, Q. V. (2021). Finetuned language models are zero-shot learners. *arXiv preprint arXiv:2109.01652.*](https://doi.org/10.48550/arXiv.2109.01652)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c42274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a Sentiment analyzer. Classify the human messages into neutral, negative, or positive sentiment. Only return the sentiment classification.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I love programming in Python!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user said, \"I love programming in Python!\" Let me analyze this. The word \"love\" is a strong positive emotion. Programming in Python is a specific activity, and the exclamation mark adds emphasis. There's no negative words or neutral terms here. So the sentiment is definitely positive.\n",
      "</think>\n",
      "\n",
      "positive\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I hate bugs in my code!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user said, \"I hate bugs in my code!\" I need to determine the sentiment here. Let's break it down. The word \"hate\" is a strong negative emotion. Bugs are a common problem for programmers, so expressing hatred towards them indicates frustration or disappointment. There's no positive language here, and while it's a specific topic, the sentiment is clearly negative. So the classification should be negative.\n",
      "</think>\n",
      "\n",
      "negative\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The sky is blue.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user said, \"The sky is blue.\" I need to determine the sentiment here. Let's think. The statement is a factual observation. It's stating a neutral fact about the color of the sky. There's no emotion attached to it. The user isn't expressing joy, anger, or any other emotion. So, the sentiment should be neutral. I should check if there's any hidden emotion, but \"blue\" is just a description. No negative or positive words here. So, neutral is the right classification.\n",
      "</think>\n",
      "\n",
      "neutral\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user said, \"The sky is blue.\" I need to determine the sentiment here. Let\\'s think. The statement is a factual observation. It\\'s stating a neutral fact about the color of the sky. There\\'s no emotion attached to it. The user isn\\'t expressing joy, anger, or any other emotion. So, the sentiment should be neutral. I should check if there\\'s any hidden emotion, but \"blue\" is just a description. No negative or positive words here. So, neutral is the right classification.\\n</think>\\n\\nneutral', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "\n",
    "instructions = \"You are a Sentiment analyzer. Classify the human messages into neutral, negative, or positive sentiment. Only return the sentiment classification.\"\n",
    "chatbot = Chatbot(llm=llm, history=[SystemMessage(content=instructions)])\n",
    "\n",
    "chatbot.invoke(\"I love programming in Python!\")  # Example prompt (positive sentiment)\n",
    "chatbot.invoke(\"I hate bugs in my code!\")  # Another example prompt (negative sentiment)\n",
    "chatbot.invoke(\"The sky is blue.\")  # Example prompt (neutral sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e816f",
   "metadata": {},
   "source": [
    "## 📚 Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting leverages the ability of Large Language Models (LLMs) to **learn a task from only a small number of labeled examples** (called *shots*), without needing large datasets.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "- The model is provided with a handful of examples (e.g., 2, 5, or 10) for each class or task.\n",
    "- Using these examples, the model generalizes to classify or predict new, unseen inputs.\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0); padding: 10px; border: 1px solid rgba(255, 255, 255, 1); border-radius: 5px; width: fit-content; margin: 10px auto;\">\n",
    "💡 This approach bridges the gap between zero-shot prompting (no examples) and full supervised learning (large labeled datasets).\n",
    "</div>\n",
    "\n",
    "\n",
    "### 📚 Reference\n",
    "\n",
    "[Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *Advances in neural information processing systems, 33, 1877-1901*.](https://doi.org/10.48550/arXiv.2005.14165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90a2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Example 1:  \n",
      "Text: \"Alice visited Paris last summer.\"  \n",
      "Entities: Alice (Person), Paris (Location)\n",
      "\n",
      "Example 2:  \n",
      "Text: \"Google was founded by Larry Page and Sergey Brin.\"  \n",
      "Entities: Google (Organization), Larry Page (Person), Sergey Brin (Person)\n",
      "\n",
      "Example 3:  \n",
      "Text: \"Amazon is headquartered in Seattle.\"  \n",
      "Entities: Amazon (Organization), Seattle (Location)\n",
      "\n",
      "Now identify entities (Organization, Person, location) in the following human messages.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Microsoft acquired LinkedIn in 2016.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user provided the sentence \"Microsoft acquired LinkedIn in 2016.\" I need to identify the entities here. First, I'll break down the sentence.\n",
      "\n",
      "The main subjects here are Microsoft and LinkedIn. The verb is \"acquired,\" which is an action, so that's not an entity. The year 2016 is a time reference, but the task is to find entities, which are Organization, Person, or Location. \n",
      "\n",
      "So, Microsoft is a company, so that's an Organization. LinkedIn is a company too, so that's also an Organization. The year 2016 is a date, but the examples given don't include dates as entities. The examples only mention entities like Person, Organization, Location. So 2016 isn't an entity here. \n",
      "\n",
      "Wait, in the examples, the third example had \"Amazon is headquartered in Seattle.\" The entities were Amazon (Organization) and Seattle (Location). So the year isn't considered an entity. So in this case, the entities are Microsoft and LinkedIn, both Organizations. \n",
      "\n",
      "I should check if there's any other possible entities. The sentence doesn't mention any people, so no Person entities. No locations mentioned either. So the answer should be Microsoft (Organization) and LinkedIn (Organization).\n",
      "</think>\n",
      "\n",
      "Entities: Microsoft (Organization), LinkedIn (Organization)\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Elon Musk is the CEO of SpaceX and Tesla.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user provided the sentence: \"Elon Musk is the CEO of SpaceX and Tesla.\" I need to identify the entities here.\n",
      "\n",
      "First, the main entities are people and organizations. The sentence mentions \"Elon Musk,\" who is a person. Then there's \"SpaceX\" and \"Tesla.\" Both of these are organizations. \n",
      "\n",
      "Wait, in the examples given, \"Google\" was an organization, and \"Larry Page\" was a person. So \"SpaceX\" and \"Tesla\" are both companies, so they should be organizations. \"Elon Musk\" is clearly a person. \n",
      "\n",
      "I should check if there's any other entity. The sentence doesn't mention any locations, so no need to include those. The answer should list Elon Musk as a Person and SpaceX and Tesla as Organizations. \n",
      "\n",
      "Let me make sure I'm not missing anything. The user's previous examples had similar structures. For instance, in the second example, \"Larry Page\" and \"Sergey Brin\" were both people, and \"Google\" was an organization. So here, \"SpaceX\" and \"Tesla\" are both organizations. \n",
      "\n",
      "Yes, that's correct. So the entities are Elon Musk (Person), SpaceX (Organization), Tesla (Organization).\n",
      "</think>\n",
      "\n",
      "Entities: Elon Musk (Person), SpaceX (Organization), Tesla (Organization)\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The Eiffel Tower is located in Paris, France.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user provided the sentence: \"The Eiffel Tower is located in Paris, France.\" I need to identify the entities here.\n",
      "\n",
      "First, the Eiffel Tower. That's a famous landmark. I remember it's a structure, so it's a Location. But wait, sometimes people might refer to it as a building, but in this context, it's a specific place, so Location makes sense.\n",
      "\n",
      "Then, Paris, France. Paris is a city, so that's a Location. France is a country, so that's also a Location. But wait, the sentence says \"located in Paris, France.\" So both Paris and France are locations. But in the examples given earlier, like in Example 3, \"Amazon is headquartered in Seattle.\" Seattle is a Location. So here, Paris is a Location, and France is a Location as well. But maybe the user expects both to be listed. Let me check the examples again.\n",
      "\n",
      "In Example 1, \"Alice visited Paris last summer.\" Paris is a Location. In Example 3, \"Amazon is headquartered in Seattle.\" Seattle is a Location. So in this case, both Paris and France are Locations. But wait, sometimes countries are considered as Locations. So the answer would be Eiffel Tower (Location), Paris (Location), France (Location). But wait, the Eiffel Tower is a specific structure, so maybe it's a Location. But sometimes, people might categorize it as a Building, but according to the examples, they categorized the Eiffel Tower as a Location. Wait, in the first example, the Eiffel Tower isn't mentioned, but in the third example, \"Amazon is headquartered in Seattle.\" So Seattle is a Location. So in this case, the Eiffel Tower is a Location, Paris is a Location, and France is a Location. But maybe the user expects only Paris as a Location, and France as a Location. But the sentence says \"located in Paris, France,\" so both are locations. So the entities would be Eiffel Tower (Location), Paris (Location), France (Location). But wait, in the examples, sometimes they list multiple locations. For example, in Example 1, Paris is a Location. In Example 3, Seattle is a Location. So here, both Paris and France are Locations. But maybe the user is expecting that. So the answer should be Eiffel Tower (Location), Paris (Location), France (Location). But wait, the Eiffel Tower is a specific structure, so maybe it's a Location. But in the examples, they didn't have that. Wait, the examples given by the user don't include a structure as an entity. Let me check again. The examples are:\n",
      "\n",
      "Example 1: Alice (Person), Paris (Location)\n",
      "\n",
      "Example 2: Google (Organization), Larry Page (Person), Sergey Brin (Person)\n",
      "\n",
      "Example 3: Amazon (Organization), Seattle (Location)\n",
      "\n",
      "So in the third example, \"Amazon is headquartered in Seattle.\" So Seattle is a Location. So in this case, the Eiffel Tower is a Location, Paris is a Location, and France is a Location. But maybe the user is expecting that. So the answer would be Eiffel Tower (Location), Paris (Location), France (Location). But wait, maybe the Eiffel Tower is a Location, and Paris is a Location, and France is a Location. But in the examples, they didn't have multiple locations in the same sentence. But in this case, the sentence has two locations: Paris and France. So the answer should be Eiffel Tower (Location), Paris (Location), France (Location). But maybe the user is expecting that. Alternatively, maybe the Eiffel Tower is a Location, and Paris is a Location, and France is a Location. So that's three entities. But in the examples, sometimes they have multiple entities. For example, in Example 2, there are three entities. So in this case, the answer would be Eiffel Tower (Location), Paris (Location), France (Location). But I need to make sure. Alternatively, maybe the Eiffel Tower is a Location, and Paris is a Location, and France is a Location. So that's correct.\n",
      "</think>\n",
      "\n",
      "Entities: Eiffel Tower (Location), Paris (Location), France (Location)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, let\\'s see. The user provided the sentence: \"The Eiffel Tower is located in Paris, France.\" I need to identify the entities here.\\n\\nFirst, the Eiffel Tower. That\\'s a famous landmark. I remember it\\'s a structure, so it\\'s a Location. But wait, sometimes people might refer to it as a building, but in this context, it\\'s a specific place, so Location makes sense.\\n\\nThen, Paris, France. Paris is a city, so that\\'s a Location. France is a country, so that\\'s also a Location. But wait, the sentence says \"located in Paris, France.\" So both Paris and France are locations. But in the examples given earlier, like in Example 3, \"Amazon is headquartered in Seattle.\" Seattle is a Location. So here, Paris is a Location, and France is a Location as well. But maybe the user expects both to be listed. Let me check the examples again.\\n\\nIn Example 1, \"Alice visited Paris last summer.\" Paris is a Location. In Example 3, \"Amazon is headquartered in Seattle.\" Seattle is a Location. So in this case, both Paris and France are Locations. But wait, sometimes countries are considered as Locations. So the answer would be Eiffel Tower (Location), Paris (Location), France (Location). But wait, the Eiffel Tower is a specific structure, so maybe it\\'s a Location. But sometimes, people might categorize it as a Building, but according to the examples, they categorized the Eiffel Tower as a Location. Wait, in the first example, the Eiffel Tower isn\\'t mentioned, but in the third example, \"Amazon is headquartered in Seattle.\" So Seattle is a Location. So in this case, the Eiffel Tower is a Location, Paris is a Location, and France is a Location. But maybe the user expects only Paris as a Location, and France as a Location. But the sentence says \"located in Paris, France,\" so both are locations. So the entities would be Eiffel Tower (Location), Paris (Location), France (Location). But wait, in the examples, sometimes they list multiple locations. For example, in Example 1, Paris is a Location. In Example 3, Seattle is a Location. So here, both Paris and France are Locations. But maybe the user is expecting that. So the answer should be Eiffel Tower (Location), Paris (Location), France (Location). But wait, the Eiffel Tower is a specific structure, so maybe it\\'s a Location. But in the examples, they didn\\'t have that. Wait, the examples given by the user don\\'t include a structure as an entity. Let me check again. The examples are:\\n\\nExample 1: Alice (Person), Paris (Location)\\n\\nExample 2: Google (Organization), Larry Page (Person), Sergey Brin (Person)\\n\\nExample 3: Amazon (Organization), Seattle (Location)\\n\\nSo in the third example, \"Amazon is headquartered in Seattle.\" So Seattle is a Location. So in this case, the Eiffel Tower is a Location, Paris is a Location, and France is a Location. But maybe the user is expecting that. So the answer would be Eiffel Tower (Location), Paris (Location), France (Location). But wait, maybe the Eiffel Tower is a Location, and Paris is a Location, and France is a Location. But in the examples, they didn\\'t have multiple locations in the same sentence. But in this case, the sentence has two locations: Paris and France. So the answer should be Eiffel Tower (Location), Paris (Location), France (Location). But maybe the user is expecting that. Alternatively, maybe the Eiffel Tower is a Location, and Paris is a Location, and France is a Location. So that\\'s three entities. But in the examples, sometimes they have multiple entities. For example, in Example 2, there are three entities. So in this case, the answer would be Eiffel Tower (Location), Paris (Location), France (Location). But I need to make sure. Alternatively, maybe the Eiffel Tower is a Location, and Paris is a Location, and France is a Location. So that\\'s correct.\\n</think>\\n\\nEntities: Eiffel Tower (Location), Paris (Location), France (Location)', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "Example 1:  \n",
    "Text: \"Alice visited Paris last summer.\"  \n",
    "Entities: Alice (Person), Paris (Location)\n",
    "\n",
    "Example 2:  \n",
    "Text: \"Google was founded by Larry Page and Sergey Brin.\"  \n",
    "Entities: Google (Organization), Larry Page (Person), Sergey Brin (Person)\n",
    "\n",
    "Example 3:  \n",
    "Text: \"Amazon is headquartered in Seattle.\"  \n",
    "Entities: Amazon (Organization), Seattle (Location)\n",
    "\n",
    "Now identify entities (Organization, Person, location) in the following human messages.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = Chatbot(llm=llm, history=[SystemMessage(content=instructions)])\n",
    "\n",
    "chatbot.invoke(\"Microsoft acquired LinkedIn in 2016.\")  # Example prompt\n",
    "chatbot.invoke(\"Elon Musk is the CEO of SpaceX and Tesla.\")  # Another example prompt\n",
    "chatbot.invoke(\"The Eiffel Tower is located in Paris, France.\")  # Example prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773db1e8",
   "metadata": {},
   "source": [
    "# 🔗 Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Chain-of-Thought (CoT) prompting is a technique that guides Large Language Models (LLMs) to **break down complex problems into smaller, logical steps**. By encouraging step-by-step reasoning, CoT helps the model arrive at more accurate and transparent solutions.\n",
    "\n",
    "For example, instead of directly giving the answer to a math problem, the model is prompted to explain the reasoning process step-by-step before concluding.  \n",
    "Example prompt:  \n",
    "`\"Think step by step and provide the final answer at the end.\"`\n",
    "\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0); padding: 10px; border: 1px solid rgba(255, 255, 255, 1); border-radius: 5px; width: fit-content; margin: 10px auto;\">\n",
    "💡 Recent LLMs like GPT-4 and LLaMA 3 now have built-in chain-of-thought capabilities as part of their cold start (e.g., via text between <code>&lt;think&gt;</code> and <code>&lt;/think&gt;</code>).  \n",
    "This means CoT reasoning happens automatically without special prompting, unless you explicitly want the model to show its intermediate steps in the final answer.\n",
    "</div>\n",
    "\n",
    "### 📚 Reference\n",
    "\n",
    "[Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. *Advances in neural information processing systems, 35, 24824-24837*.](https://doi.org/10.48550/arXiv.2201.11903)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee31ba36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant. Answer my questions in a helpful way.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the convenience store owner.\n",
      "Then I went to buy 5 more apples and ate 1. How many apples do I have left?\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user bought 10 apples initially. Then they gave 2 to the neighbor and 2 to the convenience store owner. So first, I need to subtract those from the original 10. Let me calculate that: 10 minus 2 is 8, then minus another 2 would be 6. So after giving away those apples, they have 6 left.\n",
      "\n",
      "Then they went to buy 5 more apples. So adding those to the 6 they had: 6 plus 5 equals 11. Now they have 11 apples. Then they ate 1, so subtract 1 from 11, which leaves them with 10 apples. Wait, that seems right. Let me check again. Original 10, minus 2 and 2 is 6. Then plus 5 is 11, minus 1 is 10. Yeah, that seems correct. I don't think I made a mistake here. The answer should be 10.\n",
      "</think>\n",
      "\n",
      "You started with 10 apples.  \n",
      "- Gave 2 to the neighbor: **10 - 2 = 8**  \n",
      "- Gave 2 to the convenience store: **8 - 2 = 6**  \n",
      "- Bought 5 more: **6 + 5 = 11**  \n",
      "- Ate 1: **11 - 1 = 10**  \n",
      "\n",
      "**Answer: 10 apples left.**\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"You are a helpful assistant. Answer my questions in a helpful way.\"\"\"\n",
    "prompt = \"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the convenience store owner.\n",
    "Then I went to buy 5 more apples and ate 1. How many apples do I have left?\n",
    "\"\"\"\n",
    "chatbot = Chatbot(llm=llm, history=[SystemMessage(content=instructions)])\n",
    "chatbot.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2deab4a",
   "metadata": {},
   "source": [
    "\n",
    "# 🧠 Prompt Chaining\n",
    "\n",
    "Prompt chaining is a technique used with AI language models where you **break down a complex task into a sequence of linked prompts**. Each prompt uses the output from the previous one to build a chain of reasoning or content creation.\n",
    "\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Start with an initial prompt:** Ask the model a simple or first-step question.\n",
    "2. **Use the output as context:** Feed the AI’s response into a new prompt that asks for elaboration or the next step.\n",
    "3. **Repeat as needed:** Continue chaining prompts until the final detailed answer or content is produced.\n",
    "\n",
    "## Examples\n",
    "- **Example 1:** Summarize an article → Critique the summary → Refine based on feedback.\n",
    "- **Example 2:** Generate a code → Identify bugs or inefficiencies → Refactor the code.\n",
    "\n",
    "### 📚 Reference\n",
    "\n",
    "[Wu, T., Terry, M., & Cai, C. J. (2022, April). Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In *Proceedings of the 2022 CHI conference on human factors in computing systems (pp. 1-22)*.](https://doi.org/10.1145/3491102.3517582)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c85a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Summarize this medical research paper.\n",
      "\n",
      "<paper>\n",
      "ShyPins: Safeguarding User Mental Safety From The Forcible\n",
      "(Dis)Appearance Of Pin-based Controls By Using Speed Zones\n",
      "Maxime Daniel\n",
      "Univ. Bordeaux\n",
      "ESTIA-Institute of Technology, EstiaR\n",
      "F-64210 Bidart, France\n",
      "m.daniel@estia.frWilliam Delamare\n",
      "Univ. Bordeaux\n",
      "ESTIA-Institute of Technology, EstiaR\n",
      "F-64210 Bidart, France\n",
      "w.delamare@estia.fr\n",
      "Figure 1: ShyPins regulates pin motion to prevent the forcible (dis)appearance of tangible controls, mitigating psychological\n",
      "harm to users during scenarios like reconfiguring a tangible cockpit (top) and transitioning a physical bar chart (bottom).\n",
      "Abstract\n",
      "Pin-based shape displays can cause psychological harm by\n",
      "forcefully making tangible controls (dis)appear in contact with\n",
      "users. We present ShyPins, a pin-based shape display regulating\n",
      "pin motion based on proximity to the user’s body to prevent\n",
      "such incidents. A first user study reveals that for system-triggered\n",
      "actuation, gradually decreasing pins’ speed as users get closer is\n",
      "perceived as safer than pausing pins immediately when users are\n",
      "too close. For user-triggered actuation, the perceived safety of both\n",
      "strategies depends on user preferences. However, pausing a pin\n",
      "motion creates an incoherence between the physical artifact and the\n",
      "digital data. Thus, to inform users about paused pins, the safeguard\n",
      "projects pause icons and stop zones onto the pin-based surface.\n",
      "A second study reveals that projecting user-centered stop zones\n",
      "is perceived safer than pin-centered ones. Additionally, pin-based\n",
      "This work is licensed under a Creative Commons Attribution International\n",
      "4.0 License.\n",
      "TEI ’25, March 04–07, 2025, Bordeaux / Talence, France\n",
      "©2025 Copyright held by the owner/author(s).\n",
      "ACM ISBN 979-8-4007-1197-8/25/03\n",
      "https://doi.org/10.1145/3689050.3704940surfaces are perceived as less safe when users approach pins in\n",
      "motion than when pins approach users.\n",
      "CCS Concepts\n",
      "•Human-centered computing →User studies .\n",
      "Keywords\n",
      "Shape-Changing Interfaces, Pin-Based Shape Displays, User Safety.\n",
      "ACM Reference Format:\n",
      "Maxime Daniel and William Delamare. 2025. ShyPins: Safeguarding User\n",
      "Mental Safety From The Forcible (Dis)Appearance Of Pin-based Controls\n",
      "By Using Speed Zones. In Nineteenth International Conference on Tangible,\n",
      "Embedded, and Embodied Interaction (TEI ’25), March 04–07, 2025, Bordeaux\n",
      "/ Talence, France. ACM, New York, NY, USA, 16 pages. https://doi.org/10.\n",
      "1145/3689050.3704940\n",
      "1 Introduction\n",
      "Pin-based shape displays are actuated pin arrays creating interactive\n",
      "physical surfaces: both the system and the user can pull and push\n",
      "pins to physically interact with each other [ 17,21,40,52,57].\n",
      "As demonstrated by Follmer et al. [ 17], these displays have the\n",
      "capability to make tangible controls appear or disappear on demand\n",
      "TEI ’25, March 04–07, 2025, Bordeaux / Talence, France M. Daniel and W. Delamare\n",
      "through pin actuation (buttons [ 17], dials [ 52], handles [ 17],\n",
      "joysticks [40], sliders [52]).\n",
      "However, safety is a significant challenge to address before\n",
      "allowing end-users to interact with shape-changing interfaces in\n",
      "real-world scenarios [ 4,25,43,53]. Any program can request a\n",
      "pin-based shape display, lacking proper safeguards, to make a\n",
      "tangible control appear by forcefully pushing pins into the user’s\n",
      "arm or disappear by forcefully pulling pins from the user’s hand.\n",
      "Although pin-based shape displays are designed with power and\n",
      "force limitations to prevent physical harm to the user’s body, the\n",
      "psychological impact of tangible controls forcefully (dis)appearing\n",
      "in contact with users cannot be overlooked. Indeed, incorrect or\n",
      "unexpected physical interaction of actuated devices with humans\n",
      "can evoke negative emotional responses such as distrust, discomfort,\n",
      "stress, fear, anxiety, and surprise, compromising the perceived\n",
      "safety of such devices [2, 48].\n",
      "In Robotics, a standard safety measure involves establishing\n",
      "safety speed zones around actuated devices to prevent unintended\n",
      "physical interaction with humans [ 58–60]. Our proposal extends\n",
      "this concept by implementing safety speed zones around individual\n",
      "pins to prevent the forcible (dis)appearance of tangible controls\n",
      "during user interaction. This ensures that tangible controls only\n",
      "become active when not in close proximity with users and do\n",
      "not cause psychological harm to users. Extensive research has\n",
      "investigated the impact of safety zones on the mental safety of\n",
      "human beings supervising large system-controlled devices such\n",
      "as industrial manipulators [ 2,46,48]. However, it is not clear how\n",
      "safety zones affect the well-being of humans physically interacting\n",
      "with an assembly of small user- or system-controlled devices such\n",
      "as pin-based shape displays. Furthermore, safety zones present\n",
      "a challenge when pausing pin motions, as this can introduce a\n",
      "discrepancy between the physical shape of the display (actual\n",
      "pin height) and the underlying digital data of the application\n",
      "(intended pin height). Solutions such as projecting pause icons and\n",
      "safety zones on the pin-based surface can help notify users about\n",
      "this discrepancy. Prior research has demonstrated that projecting\n",
      "safety zones around a robot effectively communicates reasons for\n",
      "slowdowns or halts [ 59,60]. Alternatively, visualizing safety zones\n",
      "around the user’s body parts rather than individual pins presents\n",
      "another approach. However, the effectiveness of one approach over\n",
      "the other in enhancing the perceived safety of pin-based shape\n",
      "displays remains unclear.\n",
      "In this paper, we introduce ShyPins , a 5x6 replica of the actuated\n",
      "pin array of the Emergeables system [ 40] enabling various tangible\n",
      "controls to (dis)appear dynamically (e.g., buttons, dials, joysticks).\n",
      "The system incorporates a safeguard that regulates pin motion\n",
      "based on their proximity to the user’s body using a motion capture\n",
      "technology. Spatial augmented reality is used to project pause icons\n",
      "and safety zones onto pins that are paused by the safeguard. A\n",
      "first lab study reveals that three speed zones (stop, half speed, full\n",
      "speed) is perceived as safer than two speed zones (stop, full speed)\n",
      "for system-triggered pin motions while the perceived safety of\n",
      "both strategies depends on user preferences for user-triggered pin\n",
      "motions. A second lab study reveals that projecting user-centered\n",
      "stop zones to inform users about paused pins is perceived as safer\n",
      "than pin-centered ones. Lastly, users approaching pins in motionis perceived as less safe than pins approaching a user’s body part.\n",
      "Our contributions are:\n",
      "(1)Knowledge transfer from Robotics focusing on ensuring the\n",
      "physical and mental safety of human beings interacting with\n",
      "actuated devices.\n",
      "(2)A comparative evaluation of the perceived safety of pin-based\n",
      "shape displays with varying safety zone numbers.\n",
      "(3)A comparative evaluation of the perceived safety of pin-based\n",
      "shape displays visualizing pin-centered safety zones or, a novel\n",
      "approach, user-centered safety zones.\n",
      "2 Motivation\n",
      "Our research aims to prevent situations where the physical controls\n",
      "of a pin-based shape display forcefully appear or vanish when\n",
      "users interact with them, which can cause psychological harm if\n",
      "safeguards are absent. Due to ethical and legal considerations [ 47],\n",
      "we did not conduct a study on negative human responses to\n",
      "faulty interactions. Instead, we present user scenarios illustrating\n",
      "how psychological harm can manifest: (1) during the physical\n",
      "reconfiguration of an adaptive cockpit, and (2) during the data\n",
      "transition of a physical bar chart.\n",
      "Scenario 1: Forcible reconfiguration of a tangible cockpit\n",
      "Sarah, a 32-year-old drone racing enthusiast, works for a\n",
      "company that specializes in rover-assisted drone maintenance\n",
      "for photovoltaic farms. The rovers recharge and deploy drones\n",
      "for solar panel inspections. Sarah trains recruits on the ShyPins\n",
      "display, which features two cockpit modes (Figure 2-a): rover and\n",
      "drone. In rover mode, a joystick with an integrated dial controls\n",
      "the rover’s xy-axis movement and z-axis rotation, while in drone\n",
      "mode, there is a joystick for xy-axis translation and z-axis rotation,\n",
      "plus a throttle for z-axis translation. During a training session,\n",
      "Sarah encounters an unexpected problem – a recent application\n",
      "update has introduced a bug. As she reaches to adjust the throttle\n",
      "in drone mode, the glitch suddenly switches the cockpit to rover\n",
      "mode, catching her off guard. In the confusion, her hand gets\n",
      "momentarily caught between shifting controls. This incident leaves\n",
      "Sarah feeling alarmed, disrupting the session and raising concerns\n",
      "about the mental impact on her and her recruits.\n",
      "Scenario 2: Forcible data transition of a physical bar chart\n",
      "Emily, a 25-year-old undergraduate passionate about sustainability,\n",
      "is drawn to the ShyPins display in her university’s entrance\n",
      "hall—a dynamic bar chart to raise awareness of campus energy\n",
      "consumption (Figure 2-b). The chart visualizes energy use across\n",
      "buildings, with bars that users manipulate to explore days and\n",
      "buildings. Eager to investigate campus energy patterns, she starts\n",
      "interacting with the display, bending bars to navigate the data.\n",
      "However, she quickly notices something strange: each time she\n",
      "bends a bar, the height of all bars, including the one she is holding,\n",
      "updates instantly. This behavior was designed for immediate\n",
      "feedback, but the sudden adjustments surprise and unsettle her.\n",
      "Initially excited, she now feels hesitant, and her unease discourages\n",
      "other passersby from engaging with the display.\n",
      "ShyPins TEI ’25, March 04–07, 2025, Bordeaux / Talence, France\n",
      "Figure 2: Psychological harm (e.g., discomfort, anxiety, mistrust) can be inflicted to users when a pin-based shape display is\n",
      "allowed to push tangible controls into the user’s arm or to pull tangible controls from the user’s hand such as during (a) the\n",
      "reconfiguration of an adaptive tangible cockpit or (b) during the data transition of a physical bar chart.\n",
      "3 Related Work\n",
      "This work builds on the literature related to (1) ensuring user safety\n",
      "with shape-changing interfaces, (2) the output capabilities of pin-\n",
      "based shape displays subject to safety considerations, and (3) the\n",
      "compliance of pin-based shape displays with safety standards.\n",
      "3.1 Ensuring User Safety With Shape-Changing\n",
      "Interfaces\n",
      "Shape-changing interfaces (SCIs) aim to blur the boundary between\n",
      "physical and virtual objects, combining the physicality of Tangible\n",
      "User Interfaces (TUIs) with the malleability of Graphical User\n",
      "Interfaces (GUIs). These actuated devices can perform physical\n",
      "transformations in response to user or system inputs, enabling\n",
      "them to convey information, meaning, or affect [ 4]. To advance SCIs,\n",
      "Alexander et al. [ 4] pinpointed twelve key challenges, including\n",
      "ensuring the safe and ethical operation of such devices [ 4]. In\n",
      "robotics, ensuring user safety has long been a priority, with \"safety\"\n",
      "referring to the well-being of humans interacting with actuated\n",
      "devices [ 2,46,48]. This includes mitigating device actions that\n",
      "could cause physical harm, such as whipping, slicing, or crushing\n",
      "(referred to as \"physical safety\"), and guarding against psychological\n",
      "harm, including feelings of distrust, discomfort, stress, fear, anxiety,\n",
      "and surprise (referred to as \"mental safety\") [ 2,46,48]. Despite\n",
      "concerns about the physical safety of users with SCIs [ 4,41,53],\n",
      "little attention has been given to this issue, especially regarding\n",
      "mental safety. Yet, the perceived safety of actuated devices is\n",
      "crucial for long-term interaction, collaboration, and acceptance [ 2]:\n",
      "comfort, predictable situations, familiar situations, sense of control,\n",
      "and trust are human factors associated with the safety perception\n",
      "of actuated devices [2].\n",
      "Device factors influencing the mental safety of humans\n",
      "include distance, speed, distance-speed proportionality, direction of\n",
      "approach, size and appearance, motion fluency and predictability,\n",
      "communication, and smooth contacts [ 2,46]. These factors prevent\n",
      "us from directly generalizing previous results involving largemanufacturing robots to small actuated devices like pin-based\n",
      "shape displays. Past research has primarily focused on system-\n",
      "controlled actuated devices for collaborative tasks [ 2,46] (e.g.,\n",
      "industrial manipulators, indoor mobile robots, humanoid robots,\n",
      "drones, autonomous vehicles). In contrast, the SCI community\n",
      "has introduced a wide variety of actuated devices for dynamic\n",
      "physical interaction, with control varying along a continuum\n",
      "between the user and the system [ 44] (e.g., sparse dots [ 30],\n",
      "single line [ 35], sparse lines [ 55], string array [ 16], hub and\n",
      "structs [ 24], voxels [ 54], layers [ 65], surfaces [ 8], and pin array [ 57]).\n",
      "Consequently, without a straightforward transfer of knowledge\n",
      "from the robotics community to the SCI community, the extent of\n",
      "user mental safety with SCIs remains uncertain.\n",
      "In our work, we investigate the mental safety of users interacting\n",
      "with actuated pin arrays, also known as pin-based shape displays.\n",
      "3.2 Output Capabilities of Pin-Based Shape\n",
      "Displays Subject to Safety Considerations\n",
      "Pin-based shape displays like InFORM [ 17] are actuated pin arrays\n",
      "creating dynamic physical surfaces: both the system and the user\n",
      "can pull and push pins to physically interact with each other.\n",
      "Previous research [ 17,45,52] has demonstrated the ability of these\n",
      "displays to dynamically render tangible controls through a three-\n",
      "stage process [37]:\n",
      "(1)Appearance – one or more pins are raised from the surface\n",
      "to form a tangible controller.\n",
      "(2)Interaction – once the tangible controller is formed, users\n",
      "can physically interact with it (e.g., touch a track [ 17], push\n",
      "a button [ 17], pull an handle [ 17], rotate a dial [ 52], move a\n",
      "slider [52], bend a joystick [45]).\n",
      "(3)Disappearance – upon completion of the interaction, the\n",
      "tangible controller disappears by retracting the set of pins\n",
      "used to create it into the surface.\n",
      "Traditionally, tangible controls on pin-based shape displays\n",
      "appear or disappear through z-axis translation of individual\n",
      "TEI ’25, March 04–07, 2025, Bordeaux / Talence, France M. Daniel and W. Delamare\n",
      "pins [ 17,45,52], or by combining z-axis translation with xy-axis\n",
      "translation and z-axis rotation of the pin matrix [52]. Importantly,\n",
      "new pin output modalities have emerged, allowing for dynamic\n",
      "tangible controls in novel ways, such as xy-axis bending of\n",
      "individual pins [ 36,38], z-axis rotation of individual pins [ 36], and\n",
      "pin inflation [ 36]. However, it remains unclear how pin output\n",
      "modalities, starting with z-axis translation, should be used to\n",
      "make tangible controls dynamically (dis)appear without causing\n",
      "psychological harm to users. For instance, a poorly implemented\n",
      "or malicious program could request a pin-based shape display to\n",
      "interrupt user interaction by suddenly making a tangible controller\n",
      "disappear through z-axis translation while being held and used\n",
      "by a human – forcible disappearance of tangible controls . Similarly,\n",
      "such a program could disrupt user interaction by suddenly making\n",
      "tangible controllers appear through z-axis translation near an\n",
      "already grasped and used tangible controller – forcible appearance\n",
      "of tangible controls . These incorrect or unexpected interactions can\n",
      "lead to negative emotional responses such as distrust, discomfort,\n",
      "stress, fear, anxiety, and surprise [2, 48].\n",
      "Previous work has focused on extending the input and\n",
      "output modalities of pin-based shape displays, with little to no\n",
      "consideration of the perceived safety of these new devices. Our\n",
      "work aims to fill this gap, starting with the most straightforward\n",
      "pin output modality, the z-axis translation, to investigate how to\n",
      "render tangible controls dynamically without causing psychological\n",
      "harm.\n",
      "3.3 Compliance of Pin-Based Shape Displays\n",
      "With Safety Standards\n",
      "Ensuring compliance with safety standards is crucial for deploying\n",
      "pin-based shape displays in real-world settings. Existing safety\n",
      "standards focus on guaranteeing the physical safety of human\n",
      "beings interacting with actuated devices, rather than their mental\n",
      "safety, which is not explicitly accounted for [ 46]. However, mental\n",
      "safety can be improved when physical safety is guaranteed,\n",
      "and thus safety standards actually contribute to improving the\n",
      "perceived safety of actuated systems [ 46]. The safety standards\n",
      "(ISO/EN: 10218-1/2 and ISO/TS 15066) define four strategies for\n",
      "actuated devices when interacting with human beings [58]:\n",
      "Two strategies for physical contact\n",
      "During physical contact with humans, the forces exerted by the\n",
      "actuated device upon the human must remain below thresholds\n",
      "to prevent any physical discomfort or injury – Power and Force\n",
      "Limiting (PFL) . For instance, a healthcare robot slowly touching\n",
      "and wiping a patient’s forearm with a force lower than 30N [ 11].\n",
      "The Power and Force Limiting (PFL) strategy creates an active\n",
      "system enabling pin actuation with low maximal speed in contact\n",
      "with and around users. The robot can also activate a passive\n",
      "mode where it compensates its weight to hold its position and get\n",
      "directly touched and guided by the user – Hand Guiding (HG) . For\n",
      "instance, a worker’s hands guiding a robotic arm to lift and attach\n",
      "an instrumental panel to a vehicle body [ 39]. The Hand Guiding\n",
      "(HG) strategy creates an active idle system, waiting for user inputs.Two strategies for collision avoidance\n",
      "Prior to physical contact with humans, a robot at maximal speed\n",
      "can immediately stop its motion if a human gets too close – Safety-\n",
      "rated Monitored Stop (SMS) . For instance, a robotic arm moving\n",
      "payloads at maximal speed behind fences immediately stopping its\n",
      "motion when an operator enters [ 58]. The robot can also gradually\n",
      "decrease its speed as the human gets closer – Speed and Separation\n",
      "Monitoring (SSM) . For instance, an autonomous vehicle adapting\n",
      "its speed to the distance from pedestrian crossings [14].\n",
      "Existing pin-based shape displays usually adopt PFL and HG\n",
      "strategies, which preserves the best level of interactivity (i.e.,\n",
      "actuation in contact with users and around them) but with\n",
      "a low level of dynamicity (i.e., low maximal speed). However,\n",
      "these pin-based shape displays can still cause fear, surprise,\n",
      "discomfort, or create an unpleasant social situation through\n",
      "incorrect or unexpected pin actuation, such as during the forcible\n",
      "(dis)appearance of tangible controls in contact with users. Our\n",
      "approach diverges from previous pin-based shape displays by\n",
      "intentionally enforcing SMS orSSM when rendering tangible\n",
      "controls dynamically. Under this approach, pins in contact with\n",
      "users remain idle and await user inputs (e.g., push, pull, bend, or\n",
      "rotate), and pin motion is restricted solely to counterbalancing their\n",
      "weight. Once out of contact with users, pin motion can occur while\n",
      "respecting safety speed zones (stop zone and full speed zone for SMS,\n",
      "and stop zone, reduced speed zone(s), full speed zone for SSM). This\n",
      "deliberate choice eliminates any forcible (dis)appearance of tangible\n",
      "controls, as individual pins are no longer capable of (dis)engaging\n",
      "physical contacts with users.\n",
      "4 ShyPins System\n",
      "ShyPins is a 5x6 pin-based shape display, inspired by the\n",
      "Emergeables system [ 45], with pins that support push, pull,\n",
      "rotation, and bending inputs, enabling dynamic (dis)appearance\n",
      "of tangible controls like buttons, dials, and joysticks. nlike\n",
      "Emergeables ,ShyPins includes a safeguard that regulates pin\n",
      "motion based on user proximity, using a Leap Motion Controller. A\n",
      "video projector is used to project pause icons and safety zones onto\n",
      "pins that are paused by the safeguard. Any application can send\n",
      "shape-change objectives to the safeguard (e.g., move pin n °25 from\n",
      "0 cm to 15 cm in 2 s) and receive notifications about shape-change\n",
      "events from the safeguard (e.g., user proximity to pin n °25 has\n",
      "changed). If the application does not adjust its objectives based on\n",
      "user proximity, the safeguard intervenes, slowing down or pausing\n",
      "pin motion.\n",
      "Design and Capabilities of Individual Pins\n",
      "Figure 3-left details the components and assembly of a ShyPins\n",
      "pin. The pin is a 10cm tall, 4cm diameter PLA cylinder, capable of\n",
      "100mm movement along the z-axis in 40 steps of 2.5mm at up to\n",
      "50mm/s. It can also detect 100mm translation along the z-axis, 360 °\n",
      "rotation around the z-axis in 15 °increments, and 45 °bending over\n",
      "the xy-axis in 1 °steps.\n",
      "Hardware and Software For Pin Array\n",
      "Figure 3-center illustrates the ShyPins assembly. The 5x6 pin\n",
      "ShyPins TEI ’25, March 04–07, 2025, Bordeaux / Talence, France\n",
      "Figure 3: Hardware and software of the ShyPins system.\n",
      "array, housed in a 50cm x 41cm x 174cm aluminum frame 85cm\n",
      "above the ground, includes a Qumi Q6 video projector 80cm above\n",
      "its rear and a Leap Motion Controller 40cm above its center. A\n",
      "surrounding white PVC panel supports additional projections. The\n",
      "software architecture, shown in Figure 3-right, uses a three-layer\n",
      "robot architecture for decisional autonomy [ 3]. Below the pin\n",
      "matrix, Five Arduino Due microcontrollers manage 150 electronic\n",
      "components via 480 wires. Each runs a C program as the functional\n",
      "layer, overseeing feedback control loops. All communicate with a\n",
      "Raspberry Pi 4 over I2C. A 750W DTX power supply powers the\n",
      "system. The Raspberry Pi 4 operates as a WiFi Access Point and\n",
      "hosts an MQTT Broker. It runs a Python script as the executive\n",
      "layer, translating objectives from the decisional layer into primitive\n",
      "functions for the functional layer. A laptop, connected to the\n",
      "video projector via HDMI and the tracking system via USB,\n",
      "joins the Raspberry Pi 4 WiFi network. The laptop communicate\n",
      "with the executive layer through the MQTT Broker using a\n",
      "publish/subscribe pattern. It runs a C# Unity script as the decisional\n",
      "layer, which includes both the safeguard and an application. The\n",
      "application sends shape-change objectives to the safeguard (e.g.,\n",
      "move pin n °25 from 0cm to 15cm in 2s) and receives notifications\n",
      "about shape-change events (e.g., user proximity to pin n °25 has\n",
      "changed). If the application does not adjust its objectives based on\n",
      "user proximity, the safeguard slows down or pauses pin motion to\n",
      "prevent psychological harm from incorrect or unexpected physical\n",
      "interactions.5 User Study 1: Perceived Safety of Pin-Based\n",
      "Shape Displays Enforcing Speed Safety Zones\n",
      "In the realm of collision avoidance strategies, the Speed and\n",
      "Separation Monitoring (SSM) strategy (three or more speed zones)\n",
      "is known to enhance the perceived safety of large actuated devices\n",
      "compared to the Safety-rated Monitored Stop (SMS) strategy (two\n",
      "speed zones): Mobile service robots [ 9], autonomous cars [ 14], and\n",
      "industrial manipulators [ 28] are perceived as safer when slowing\n",
      "down as they approach humans. However, these findings do not\n",
      "directly apply to pin-based shape displays, where users physically\n",
      "interact with small actuated pins. Various factors, such as the\n",
      "robot’s appearance [ 46], size [ 23,42], and user control [ 2]) influence\n",
      "perceived safety [ 2,46]. Therefore, results from large robots cannot\n",
      "be directly transferred to small actuated devices like an actuated\n",
      "pin matrix [2, 46, 48].\n",
      "In Human-Computer Interaction (HCI), immediate feedback is\n",
      "crucial for ensuring the user’s sense of control and maintaining\n",
      "focus [ 41]. With SSM (three or more speed zones), more pins are\n",
      "affected, slowing the overall matrix actuation around the user\n",
      "compared to SMS (two speed zones), which can negatively impacts\n",
      "the overall dynamicity of the system. Thus, it is crucial to investigate\n",
      "how the number of speed zones impacts the perceived safety of\n",
      "small robots.\n",
      "To address this, we conducted a first user study to examine the\n",
      "impact of varying the number of pin speed zones on the perceived\n",
      "safety and dynamicity of pin-based shape displays. This study\n",
      "considered both user-initiated and system-triggered pin motions,\n",
      "such as during the forcible (dis)appearance of tangible controls.\n",
      "TEI ’25, March 04–07, 2025, Bordeaux / Talence, France M. Daniel and W. Delamare\n",
      "Figure 4: Factors of the first study: the number of pin speed zones (a), the level of user body engagement (b), and the trigger of\n",
      "pin actuation (c).\n",
      "5.1 Factors\n",
      "5.1.1 Number of Speed Safety Zones. The ShyPins safeguard\n",
      "perceives the user as virtual spheres and cylinders (i.e., the user\n",
      "zone) enclosing respectively the skeletal hands and forearms\n",
      "generated by the Leap Motion Controller. A radius offset of 3 cm\n",
      "is added to the user zone to (1) ensure that the complete hands\n",
      "and forearms of the user are enclosed and (2) compensate for the\n",
      "tracking error of the Leap Motion Controller [ 61]. The safeguard\n",
      "restricts the maximum speed of each pin using the following\n",
      "equation:\n",
      "𝑆𝑟𝑒𝑠𝑡𝑟𝑖𝑐𝑡𝑒𝑑 =𝑚𝑎𝑥(𝑓𝑙𝑜𝑜𝑟(𝐷𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑\n",
      "𝐷𝑚𝑖𝑛)×1\n",
      "𝑁×𝑆𝑚𝑎𝑥,𝑆𝑚𝑎𝑥)\n",
      "where𝐷𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑 is the distance from the pin’s edge to the user’s\n",
      "safety zone, 𝐷𝑚𝑖𝑛is the minimal separation distance between a\n",
      "moving pin and the user zone, 𝑆𝑚𝑎𝑥 is the maximum speed of\n",
      "a ShyPins pin (50mm/s), and 𝑁is the number of zones. We use\n",
      "a minimal separation distance of 3cm ( 𝐷𝑚𝑖𝑛=30mm) which\n",
      "corresponds to the space between the center of one pin and the edge\n",
      "of an adjacent pin. For each standard collision avoidance strategy,\n",
      "we use the following number of discrete separation zones based on\n",
      "common practices in the literature [58] (Figure 4a):\n",
      "•(𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 ) Two separation zones for the Safety-rated Monitored\n",
      "Stop (SMS) strategy (𝑁=2, pause zone, full speed zone).\n",
      "•(𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 ) Three separation zones for the Speed and Separation\n",
      "Monitoring (SSM) strategy (𝑁=3, pause zone, half speed zone,\n",
      "full speed zone).\n",
      "In the worst case scenario, the safeguard halts a ShyPins pin motion\n",
      "at maximum speed (50mm/s) within 15mm in 0.23s.\n",
      "5.1.2 Level of User Body Engagement. We consider the engagement\n",
      "of the user body over the pin-based surface. Indeed, as the user\n",
      "zone covers more area of the pin matrix, more pins can potentiallysurround the user’s body parts, and hence impacts perceived safety\n",
      "(distance factor [46]). We consider three levels (Figure 4b):\n",
      "•(𝐿𝑒𝑣𝑒𝑙𝑓𝑖𝑛𝑔𝑒𝑟𝑠 ) The user engages with their fingers (i.e., target pin\n",
      "in the first row of the matrix).\n",
      "•(𝐿𝑒𝑣𝑒𝑙ℎ𝑎𝑛𝑑 ) The user engages with their hand (i.e., target pin is\n",
      "in the middle row of the matrix).\n",
      "•(𝐿𝑒𝑣𝑒𝑙𝑓𝑜𝑟𝑒𝑎𝑟𝑚 ) The user engages with their forearm (i.e., target\n",
      "pin in the last row of the matrix).\n",
      "5.1.3 Trigger of Pin Actuation. We consider the pin matrix\n",
      "actuation trigger. Indeed, users might perceive safety differently if\n",
      "the shape animation is expected or not ( predictability factor [ 46]).\n",
      "We consider two levels (Figure 4c):\n",
      "•(𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑢𝑠𝑒𝑟) The shape-change is triggered via user input, and\n",
      "hence expected.\n",
      "•(𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑠𝑦𝑠𝑡𝑒𝑚 ) The shape-change is triggered by the system,\n",
      "and hence unexpected.\n",
      "5.2 Experimental Task\n",
      "We consider a task with pin actuation triggered either by user\n",
      "input or by system event [ 44]. The application starts with all pins\n",
      "colored in white in their lowest position. Only the target pin is\n",
      "set to its highest position and displays a black pointer and a red\n",
      "arc, both randomly oriented. Participants have to grasp the target\n",
      "pin, and rotate it so that the pointer enters the red arc (Figure 4).\n",
      "In the𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑢𝑠𝑒𝑟 condition, the application requests an unsafe\n",
      "shape-change as soon as the pointer enters the arc. Participants\n",
      "were asked to keep their hand on the target pin until all moving\n",
      "pins not paused by the safeguard complete their motion. In the\n",
      "𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑠𝑦𝑠𝑡𝑒𝑚 condition, the application requests an unsafe shape-\n",
      "change after a delay between 5s and 15s after the pointer enters the\n",
      "arc to create an unexpected matrix activation. To simulate a primary\n",
      "task disturbed by a system event, the red arc starts moving once\n",
      "ShyPins TEI ’25, March 04–07, 2025, Bordeaux / Talence, France\n",
      "the pointer enters, and participants have to maintain the pointer in\n",
      "the arc before and during the shape-change. Then, the application\n",
      "requests the interface to display all pins in black until there is no\n",
      "more human body presence over the pin array.\n",
      "The unsafe shape-change simulates a worst-case scenario: the\n",
      "system tries to remove the target pin from the participant’s hand,\n",
      "and moves all the other pins to their maximal position (10cm above\n",
      "the surface) at maximal speed (50mm/s). If not mediated by the\n",
      "safeguard, moving pins would entrap the user’s hand, and hit user’s\n",
      "fingers, palm, and forearm.\n",
      "5.3 Participants\n",
      "We recruited 16 participants (8 females, 8 males) on the university\n",
      "campus through mailing lists and public posters. Participation in\n",
      "this study was voluntary and unpaid. Participants had a mean\n",
      "age of 32.5±10.9years, with a median of 5 years of university\n",
      "education. One participant had previous experience with shape-\n",
      "changing interfaces or robots, having programmed a robotic arm\n",
      "during a student project.\n",
      "5.4 Apparatus and Procedure\n",
      "The experiment lasted approximately one hour per participant.\n",
      "We introduced participants to the context of safety with pin\n",
      "matrix actuation, the ShyPins system, the objectives to compare\n",
      "two numbers of pin speed zones, and the procedure of the\n",
      "experimentation. We then asked participants to read and to sign\n",
      "an informed consent form, and to fill out a demographic form.\n",
      "After verbal approval, we equipped the Empatica E4 wristband\n",
      "on their non-dominant arm to measure information about stress,\n",
      "fear, anxiety or surprise [ 46]. Participants wore noise-cancelling\n",
      "headphones to reduce interface’s mechanical noise known for\n",
      "adding mental stress [ 64]. This was done to ensure that we could\n",
      "accurately measure the mental stress generated solely by pin\n",
      "motions, rather than by the noise.\n",
      "The experiment was conducted in two phases – one for each\n",
      "trigger. Within each phase, the participants completed two sessions\n",
      "– one for each number of pin speed zones. Each session included nine\n",
      "trials – three trials for each level of user body engagement. In total,\n",
      "participants performed 𝑇𝑟𝑖𝑔𝑔𝑒𝑟 2×𝑍𝑜𝑛𝑒 2×𝐿𝑒𝑣𝑒𝑙 3×𝑇𝑟𝑖𝑎𝑙 3=36trials.\n",
      "A Latin square design was employed to counterbalance the order\n",
      "of trigger, and the order of pin speed zone number for each trigger,\n",
      "addressing potential learning and familiarization effects. The order\n",
      "of user body engagement levels was randomized for each session.\n",
      "To accommodate both left-handed and right-handed participants,\n",
      "the position of the target pin follows a random sequence including\n",
      "three positions (1 per trial): the second column, the fifth column,\n",
      "and the third/fourth columns. Before each trigger, participants were\n",
      "introduced to the task using the ShyPins system and underwent\n",
      "a training session to practice rotating the target pin and aiming\n",
      "the cursor at the red arc (i.e., 9 trials without shape-change). To\n",
      "ensure participants’ physiological signals were at rest before each\n",
      "session, a 3-minute relaxation video was played using a flat screen\n",
      "and headphones.5.5 Variables\n",
      "To assess users’ perceived safety with ShyPins , we employed a multi-\n",
      "method approach, as different assessment methods can provide a\n",
      "more reliable evaluation of perceived safety [ 15,46]. The methods\n",
      "used are:\n",
      "•System events. During each trial , we logged user inputs (e.g., pin\n",
      "rotation), shape states (e.g., pin position), and application events\n",
      "(e.g., shape-change triggered). This data provides a record of\n",
      "interactions and system responses.\n",
      "•Physiological signals. We recorded two physiological signals\n",
      "during each trial : the electrodermal activity (EDA) and the blood\n",
      "volume pulse (BVP). Using the NeuroKit2 Python library [ 33], we\n",
      "performed an event-related analysis of these signals: we extracted\n",
      "the maximum Skin Conductance Response (SCR) and the average\n",
      "Heart Rate (HR) features from signals sampled from -1s to +6s\n",
      "after each shape-change.\n",
      "•Questionnaires. After each session , participants completed three\n",
      "items from the GodSpeed questionnaire [ 7] regarding the\n",
      "perceived safety of the number of pin speed zones.\n",
      "•Interviews. After each phase , we asked participants for their\n",
      "preferences and feedback on the two numbers of pin speed zones:\n",
      "\"Do you have a preference for one of the two sessions you just\n",
      "completed? If so, which one and why?\". Then, we used an affinity\n",
      "diagram [ 32] to categorize user feedback and identify common\n",
      "perceptions.\n",
      "5.6 Results\n",
      "We removed 35 trials (6%) from the analysis due to missing\n",
      "physiological recordings. Non-parametric tests (Friedman and\n",
      "Wilcoxon tests) were used, with all post-hoc tests reporting\n",
      "Bonferroni corrected p-values. Significant test results are reported\n",
      "(𝛼≤0.05) with their effect sizes ([F(riedman) or W(ilcoxon) statistic,\n",
      "effect size, p-value]).\n",
      "We first verified that participants managed to maintain the\n",
      "cursor in the moving red arc during the primary task of the\n",
      "system-triggered pin actuation condition. After a system-triggered\n",
      "pin actuation, users managed to keep the cursor in the moving\n",
      "red arc for 88.69% [86.83, 90.54] of the pin animation time with\n",
      "𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 and for 89.28% [87.70, 90.86] of the pin animation time\n",
      "with𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 (𝑝>0.05).\n",
      "5.6.1 Level of User Body Engagement. We did not find any\n",
      "significant results regarding the level of user body engagement\n",
      "on any of the dependent variables (all 𝑝>0.05). We hypothesize\n",
      "that the pin matrix of our current prototype may be too small\n",
      "and/or too coarse to create a significant effect on perceived safety,\n",
      "regardless of how far participants’ arms extend above the surface.\n",
      "5.6.2 Trigger of Pin Actuation. We found a significant main effect of\n",
      "𝑇𝑟𝑖𝑔𝑔𝑒𝑟 on all perceived safety items. Participants reported feeling\n",
      "significantly more relaxed [W=59, Z=0.572, p=0.012], more calm\n",
      "[W=59.0, Z=0.572, p=0.012], and less surprised [W=56, Z=0.63,\n",
      "p=0.006] with 𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑢𝑠𝑒𝑟 than with𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑠𝑦𝑠𝑡𝑒𝑚 (Figure 5a).\n",
      "This is supported by skin conductance responses, significantly\n",
      "greater [W=13160, Z=0.2, p=0.006] with 𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑠𝑦𝑠𝑡𝑒𝑚 than with\n",
      "𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑢𝑠𝑒𝑟 (Figure 5b). However, no significant difference was\n",
      "TEI ’25, March 04–07, 2025, Bordeaux / Talence, France M. Daniel and W. Delamare\n",
      "Figure 5: Results of pin actuation triggers ( 𝑇𝑟𝑖𝑔𝑔𝑒𝑟 ) and pin speed zone numbers ( 𝑍𝑜𝑛𝑒 ) on perceived safety items (a) and\n",
      "physiological signals (b).\n",
      "found between 𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑢𝑠𝑒𝑟 and𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑠𝑦𝑠𝑡𝑒𝑚 in average heart\n",
      "rate [W=16728, Z=-0.11, p=0.11].\n",
      "The same effects appear during 𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 : Participants\n",
      "reported feeling significantly more relaxed [W=0.0, Z=1.0, p=0.005],\n",
      "more calm [W=0.0, Z=1.0, p=0.03], and less surprised [W=0.0, Z=1.0,\n",
      "p<0.001] with 𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑢𝑠𝑒𝑟 than with𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑠𝑦𝑠𝑡𝑒𝑚 (Figure 5a).\n",
      "However, during 𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 , we did not find a significant difference\n",
      "between𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑢𝑠𝑒𝑟 and𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑠𝑦𝑠𝑡𝑒𝑚 on the maximal skin\n",
      "conductance response [F=4429, Z=-0.03, p=1.0], the average heart\n",
      "rate [F=4269, Z=-0.16, p=0.40], the level of relaxation [F=28, Z=0.15,\n",
      "p=1.0], the level of surprise [F=37, Z=0.19, p=1.0], or the level of\n",
      "agitation [W=16, Z=-0.11, p=1.0].\n",
      "These findings validate previous results from the robotics\n",
      "community, which show that a sense of control positively impacts\n",
      "perceived safety [ 2]. We will next analyze how the perceived safety\n",
      "with each number of pin speed zones is influenced by the actuation\n",
      "trigger.\n",
      "5.6.3 Number of Speed Safety Zones. We did not find any\n",
      "significant result regarding the pin speed zone number factor\n",
      "on any dependent variables (all 𝑝>0.05). Indeed, it appears\n",
      "that with 𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑢𝑠𝑒𝑟 (i.e., participants in total control of\n",
      "the shape-changing event), both numbers of pin speed zones\n",
      "are acceptable. The only significant difference between the\n",
      "numbers of pin speed zones was observed with 𝑇𝑟𝑖𝑔𝑔𝑒𝑟𝑠𝑦𝑠𝑡𝑒𝑚 :\n",
      "participants reported feeling significantly more calm [W=0.0,\n",
      "Z=-1, p=0.04], and less surprised [W=0.0, Z=-1, p=0.04] with\n",
      "𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 than with 𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 (Figure 5a). This finding is\n",
      "supported by skin conductance responses, which were significantly\n",
      "greater [W=2453, Z=0.32, p<0.01] with 𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 than with\n",
      "𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 (Figure 5b). However, no significant difference was foundbetween𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 and𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 in average heart rate [W=4119,\n",
      "Z=-0.03, p=1.0].\n",
      "These results validate previous findings from the robotics\n",
      "community [ 2,46], demonstrating that SSM (three or more speed\n",
      "zones) – with slowed down pins – is perceived as better than SMS\n",
      "(two speed zones). However, our results also show that with user-\n",
      "triggered pin motions, there is no significant difference between\n",
      "SSM (three or more speed zones) and SMS (two speed zones). This\n",
      "suggests that SMS could potentially improve dynamicity by not\n",
      "slowing down extra pins around the user’s body part.\n",
      "5.7 Study 1 Discussion\n",
      "We summarize the results and discuss the main takeaways in light\n",
      "of participants’ feedback.\n",
      "5.7.1 With system-triggered pin motions: Using three speed zones\n",
      "rather than two speed zones improves the perceived safety. Enforcing\n",
      "three speed zones is (1) more calm, (2) less surprising and (3) induces\n",
      "less stress than enforcing two speed zones with system-triggered\n",
      "pin motions. Thirteen participants (81.13%) preferred three speed\n",
      "zones, which slows down and soothes pin motion around the user\n",
      "body, compared to two speed zones (e.g., P5:\"During the last session\n",
      "[𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 ], the system was faster and more brutal because all pins\n",
      "moved together. \" ).\n",
      "Six participants (37.5%) raised that the focus of attention also\n",
      "impacts perceived safety. Indeed, while performing a task in the\n",
      "focus of attention, users were more surprised, distracted, stressed\n",
      "and/or scared by pin matrix actuation happening in their periphery\n",
      "of attention with two speed zones than with three speed zones (e.g.,\n",
      "P2: \"I preferred earlier [ 𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 ] because I was less surprised by\n",
      "the system when I was focused [on the task]. I was more surprised\n",
      "here [𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 ]. \").\n",
      "ShyPins TEI ’25, March 04–07, 2025, Bordeaux / Talence, France\n",
      "These results interconnect previous works on perceived safety\n",
      "and peripheral interaction supporting (1) that a system-controlled\n",
      "robot motion is perceived safer if it slows down as it approaches\n",
      "humans [ 46], and (2) that a system-controlled robot motion in user’s\n",
      "periphery of attention should be slow and quiet to keep the user\n",
      "focused on the main task [ 12]. To improve the perceived safety of\n",
      "pin-based shape displays rendering tangible controls dynamically,\n",
      "safeguards should enforce three rather than two speed zones on pin\n",
      "matrix actuation triggered by a system event, especially in user’s\n",
      "periphery of attention.\n",
      "5.7.2 With user-triggered pin motions: Using two or three speed\n",
      "zones depends on user preferences. When pin motion is triggered\n",
      "by a user input, the results reveals that:\n",
      "•Eight participants (50%) preferred three speed zones for a calm\n",
      "shape-change completion (e.g., P11: \"During the first session\n",
      "[𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 ], it was smoother and less stressful than during the last\n",
      "one [𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 ]: I like having time to see the motion coming.\" ).\n",
      "The different pin speeds also created an overall pleasant effect\n",
      "with𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 (e.g., P1:\"In the last session [ 𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 ], the robot\n",
      "was smoother and more pleasant to watch [than 𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 ]. \").\n",
      "•The other eight participants preferred two pin speed zones for\n",
      "a fast shape-change completion (e.g., P8: \"I preferred the first\n",
      "session [𝑍𝑜𝑛𝑒𝑑𝑜𝑢𝑏𝑙𝑒 ] because the system was faster and it did not\n",
      "bother me to wait [until all pin motions were completed around\n",
      "me]. However, I waited too much time during the second session\n",
      "[𝑍𝑜𝑛𝑒𝑡𝑟𝑖𝑝𝑙𝑒 ] and it annoyed me. \" ).\n",
      "These results on pin-based surfaces echo with previous studies\n",
      "arguing that individual human characteristics (personality, gender,\n",
      "experience, culture) and preferences are important factors for the\n",
      "safety perception of actuated devices [ 46]. To improve the perceived\n",
      "safety of pin-based shape displays rendering tangible controls\n",
      "dynammically, the safeguard should enforce three speed zones\n",
      "to trigger calm pin matrix actuation motion. For users comfortable\n",
      "with the system, the safeguard could use only two speed zones to\n",
      "allow fast shape-change completion, and hence favor dynamicity.\n",
      "5.7.3 Moving Forward: Addressing Incoherence in Pin State. If\n",
      "the application fails to adjust pin motion to user proximity, the\n",
      "safeguard intervenes by slowing down or pausing pins in the\n",
      "safety zones around the user’s body. This situation, especially pin\n",
      "motion in pause, creates an incoherence as the physical model of\n",
      "the interface does not match the virtual model of the application\n",
      "anymore. The safeguard should inform users about pins in an\n",
      "incoherent state and the reasons for this state. The follow-up study\n",
      "focuses on visualization techniques to inform users about paused\n",
      "pins and speed zones. In this follow-up study, we enforced three\n",
      "safety speed zones (SSM) due to its superior perceived safety in\n",
      "system-triggered scenarios, and no significant difference in user-\n",
      "triggered scenarios.\n",
      "6 Study 2: Perceived Safety Of Pin-Based Shape\n",
      "Displays Using Stop Zone Visualizations\n",
      "Collision avoidance strategies can create discrepancies between\n",
      "the physical shape of the display (actual pin height) and the\n",
      "corresponding digital data of the application (intended pin height).\n",
      "Prior research has shown that projecting safety zones around arobot effectively communicates reasons for slowdowns or halts [ 59,\n",
      "60]. Thus, projecting pause icons and safety zones on the pin-based\n",
      "surface can help notify users about these discrepancies. However,\n",
      "the robotics community recognizes the need for a more user-\n",
      "centered approach to safety [ 13]. In HCI, visualizing safety zones\n",
      "around user’s body parts, rather than individual pins, could offer a\n",
      "novel approach. The effectiveness of this method in enhancing the\n",
      "perceived safety of pin-based shape displays remains to be explored.\n",
      "We conducted a second study to compare the perceived safety of\n",
      "user-centered versus pin-centered stop zone visualizations on pin-\n",
      "based shape displays. This comparison focused on situations where\n",
      "the safeguard needs to inform users about paused pin motions, such\n",
      "as during the forcible (dis)appearance of tangible controls.\n",
      "6.1 Factors\n",
      "6.1.1 Stop Zone Visualization. We designed two visualization\n",
      "techniques to project information onto the pin-based surface,\n",
      "informing users about paused pins and explaining why these pins\n",
      "have been paused. Both techniques (Figure 6a) project a red dot and\n",
      "a white pause icon on each paused pin, along with a 1cm-wide red\n",
      "circle representing the minimal separation distance ( 𝐷𝑚𝑖𝑛radius):\n",
      "•(𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛) Displays a stop zone around each paused pin ( pin-\n",
      "centered approach ).\n",
      "•(𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟) Displays a stop zone around the user’s body ( user-\n",
      "centered approach ).\n",
      "Both visualizations represent the same minimal separation distance\n",
      "between the user and the pin ( 𝐷𝑚𝑖𝑛), known as the stop zone.\n",
      "However, while 𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟 displays a single stop zone around the\n",
      "user’s body, 𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛displays a stop zone for each paused pin.\n",
      "To prevent visual overload, 𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛removes the inner edges of\n",
      "intersecting zones.\n",
      "Design Rationale. Both visualizations are based on the safety\n",
      "overlay proposed by Vogel et al. [ 60], which uses a 4 cm-wide red\n",
      "line to indicate the stop zone around an industrial robot (Figure 7-\n",
      "left). We adapted the overlay for ShyPins to reduce occlusion\n",
      "with the application’s visuals. For 𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛(Figure 7-right), the\n",
      "adjustments were: (1) Removed inner edges of overlapping stop\n",
      "zones around paused pins to reduce visual clutter, adding a \"pause\"\n",
      "icon for clarity; (2) Reduced the red line thickness from 4 cm to 1\n",
      "cm to avoid covering a 4 cm-diameter pin; (3) Added a 25 mm-wide\n",
      "white outline to ensure contrast when application visuals included\n",
      "red tones. For 𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟 (Figure 7-center), we applied the same\n",
      "overlay but visualized the stop zone around the user zone instead\n",
      "of individual pins.\n",
      "6.1.2 Pin Matrix Actuation Density. We consider the density of the\n",
      "pin matrix actuation as a second factor, as it defines the number of\n",
      "moving pins that can potentially be paused ( sizefactor [ 46]). We\n",
      "define three density levels, excluding the target pin (Figure 6b):\n",
      "•(𝐷𝑒𝑛𝑠𝑖𝑡𝑦 33%) 9 pins are actuated, evenly distributed across the\n",
      "matrix.\n",
      "•(𝐷𝑒𝑛𝑠𝑖𝑡𝑦 66%) 19 pins are actuated, evenly distributed across the\n",
      "matrix.\n",
      "•(𝐷𝑒𝑛𝑠𝑖𝑡𝑦 100%) the remaining 29 pins in the matrix are actuated.\n",
      "6.1.3 Shape-Change Interruption. We explore two instances of\n",
      "shape-change interruption events (Figure 6c), where either the\n",
      "TEI ’25, March 04–07, 2025, Bordeaux / Talence, France M. Daniel and W. Delamare\n",
      "Figure 6: Factors of the second study: the stop zone visualization (a), the pin matrix actuation density (b), and the shape-change\n",
      "interruption event (c).\n",
      "Figure 7: Design of the 𝑉𝑖𝑠𝑢𝑎𝑙 factor.\n",
      "system commands or the user movements induce a pause in pin\n",
      "motion ( predictability factor [46]):\n",
      "•(𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑢𝑠𝑒𝑟) The user intentionally brings their body within\n",
      "the minimum separation distance from moving pin(s). For\n",
      "example, if the application is moving a pin upward and the user\n",
      "places a hand above it (i.e., user approaching moving pins).•(𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑠𝑦𝑠𝑡𝑒𝑚 ) The application intentionally causes pin(s) to\n",
      "come within the minimum separation distance from a user body\n",
      "part. For example, if the user already has a hand above a pin\n",
      "and the application requests the pin to move upward (i.e., pins\n",
      "approaching the user).\n",
      "6.2 Experimental Task\n",
      "We use the same task as study 1. Participants are required to reach\n",
      "a target pin, which displays a black pointer and a red arc, and\n",
      "adjust the pin’s rotation to position the pointer within the red arc\n",
      "(Figure 6). Participants were informed that the task was a pretext\n",
      "for interacting with the system and were instructed to focus on the\n",
      "shape change, moving and paused pins, and zone visualizations. In\n",
      "the𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑢𝑠𝑒𝑟 condition, the system triggers the shape change\n",
      "at the beginning of the trial. Participants must reach the target pin\n",
      "(among the moving pins) during the shape change, which requires\n",
      "them to navigate around moving pins and the target pin itself to\n",
      "complete the rotation task. In the 𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑠𝑦𝑠𝑡𝑒𝑚 condition, the\n",
      "system triggers the shape change 6 to 15 seconds after participants\n",
      "begin rotating the target pin. The system continuously rotates\n",
      "the arc, requiring participants to adjust the pointer’s rotation\n",
      "during the shape change, which results in interruptions from pins\n",
      "getting closer to their forearm and hand. The target pin is randomly\n",
      "positioned in the fourth row of the pin matrix to ensure that at least\n",
      "one pin will be paused under the user’s forearm. In all conditions,\n",
      "pin motion duration is set to 6 seconds to give participants enough\n",
      "time to reach the target.\n",
      "6.3 Participants\n",
      "We recruited 16 participants (8 females, 8 males) on the university\n",
      "campus through mailing lists and public posters. Participation in\n",
      "ShyPins TEI ’25, March 04–07, 2025, Bordeaux / Talence, France\n",
      "this study was voluntary and unpaid. None of the participants had\n",
      "been involved in the first user study The participants had a mean\n",
      "age of 30.3±6.5years old, with a median of 5 years of university\n",
      "education. None had prior experience with physical interactions\n",
      "involving shape-changing interfaces or robots.\n",
      "6.4 Apparatus and Procedure\n",
      "The apparatus and procedure were identical to those used in\n",
      "the first study Each participant went through two phases – one\n",
      "for each interruption type. In each phase, participants completed\n",
      "twosessions – one for each visualization technique. Within each\n",
      "session, participants performed nine trials – three trials for each\n",
      "density level. This results in a total of 𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡 2×𝑉𝑖𝑠𝑢𝑎𝑙 2×\n",
      "𝐷𝑒𝑛𝑠𝑖𝑡𝑦 3×𝑇𝑟𝑖𝑎𝑙 3=36trials per participant. Trials were randomly\n",
      "ordered within each session. To accommodate both left-handed\n",
      "and right-handed participants, the target pin’s position followed a\n",
      "random sequence across three locations (one per trial): the second\n",
      "column, the fifth column, and the third/fourth columns. To ensure\n",
      "participants’ physiological signals were at rest before each session,\n",
      "a 3-minute relaxation video was played using a flat screen and\n",
      "headphones.\n",
      "6.5 Variables\n",
      "We reused the dependent variables of the first study, with the\n",
      "exception of the questionnaires. Since visual factors (i.e., stop zone\n",
      "visualizations) might have more subtle effects on users’ safety\n",
      "perception compared to physical factors (e.g., collision avoidance\n",
      "strategies), we replaced the GodSpeed questionnaire with the\n",
      "more sensitive Perceived Safety Questionnaire of Akalin et al. [ 1].\n",
      "This questionnaire includes eight items rated on a 5-points Likert\n",
      "scale: our items assess participants’ feelings during the interaction\n",
      "(e.g., anxious/relaxed), and four items characterize the robot itself\n",
      "(e.g., threatening/safe). Additionally, we employed the Short User\n",
      "Experience Questionnaire (UEQ-S [ 50]), which includes eight items\n",
      "on a 7-points Likert scale. The UEQ-S assesses both the hedonic\n",
      "quality (four items, e.g., boring/exciting) and the pragmatic quality\n",
      "(four items, e.g., confusing/clear) of each visualization technique.\n",
      "6.6 Results\n",
      "No trials were removed from the analysis, as all recordings were\n",
      "complete. We applied the same statistical analysis and reporting\n",
      "methods as in the first study. We first verified that participants\n",
      "managed to perform the primary task of the system-triggered\n",
      "interruption condition. Users successfully maintained the cursor in\n",
      "the moving red arc for 87.69% [82.52, 92.86] of the pin animation\n",
      "time with𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛and for 91.70% [88.17, 95.23] of the pin\n",
      "animation time with 𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟 (𝑝>0.05).\n",
      "6.6.1 Density Level. We did not find any significant effects of\n",
      "density level on any of the dependent variables (all 𝑝>0.05). We\n",
      "hypothesize that, similar to the first study, the current prototype’s\n",
      "pin matrix may be either too small or too coarse to significantly\n",
      "impact perceived safety.\n",
      "6.6.2 Interruption Type. The interruption type did not have a\n",
      "significant effect on the questionnaire results (all 𝑝>0.05).\n",
      "However, there was a significant main effect of 𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡 onaverage heart rate, with participants’ heart rates significantly\n",
      "greater [W=14592.0, Z=-0.299, p<0.001] during 𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑢𝑠𝑒𝑟 than\n",
      "during𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑠𝑦𝑠𝑡𝑒𝑚 (Figure 8c). No significant difference was\n",
      "found in skin conductance response between 𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑢𝑠𝑒𝑟 and\n",
      "𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑠𝑦𝑠𝑡𝑒𝑚 [W=15478, Z=-0.05, p=0.47].\n",
      "6.6.3 Visualization Technique. No significant difference was found\n",
      "between𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟 and𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛in skin conductance response\n",
      "[W=15231, Z=-0.07, p=0.30] or in heart rate [W=20406, Z=-0.02,\n",
      "p=0.78]. However, a significant main effect of 𝑉𝑖𝑠𝑢𝑎𝑙 was observed\n",
      "on four perceived safety items and four user experience items.\n",
      "Participants felt significantly more comfortable [W=45, Z=0.571,\n",
      "p=0.022] and more in control [W=34, Z=0.602, p=0.023] with\n",
      "𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟 than with𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛. In addition, participants perceived\n",
      "the interface as significantly more familiar [W=22.5, Z=0.669,\n",
      "p=0.011] and more calm [W=18.0, Z=0.735, p=0.008] with\n",
      "𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟 than with𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛(Figure 8a).\n",
      "When it comes to inform users about paused pins and stop\n",
      "zones, participants significantly characterized the interface as\n",
      "more supportive [W=44.0, Z=0.62, p=0.012], easier to use [W=22.5,\n",
      "Z=0.78, p=0.002], more efficient [W=35.5, Z=0.74, p=0.001], and\n",
      "clearer [W=57.5, Z=0.617, p=0.008] with 𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟 than with\n",
      "𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛(Figure 8b).\n",
      "6.7 Study 2 Discussion\n",
      "We summarize the results and discuss the main takeaways in light\n",
      "of participants’ feedback.\n",
      "6.7.1 Projecting user-centered stop zones rather than pin-centered\n",
      "ones to inform users about paused pins. Auser-centered stop zone\n",
      "follows a single user’s body part to reveal its effect range on paused\n",
      "pins, while a pin-centered zone shows the effect range of a single\n",
      "paused pin relative to the user’s body part. User-centered stop\n",
      "zones, the novel approach proposed in this paper:\n",
      "(1)Are preferred by participants. Eleven participants (68.75%)\n",
      "preferred𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟 to𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛(e.g., P14: \"In the first session\n",
      "[𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟], I had only one circle to watch while I had as many\n",
      "circles to watch as there was cylinders in pause in the last session\n",
      "[𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛].\"). Note that three participants (18.75%) preferred\n",
      "𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛, which can ease the detection of paused pins (three\n",
      "participants, e.g., P11: \"In the last session [ 𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛], there\n",
      "was a direct circle around the cylinders in pause and it was more\n",
      "visible. \" ).\n",
      "(2)Improve user’s feeling of safety (four participants, e.g., P3: \"I\n",
      "felt more secured in the last session [ 𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟] because I had\n",
      "the feeling that nothing could happen to me inside the zone.\" ) ,\n",
      "and feeling of control (four participants, e.g., P0: \"I found the\n",
      "system more controllable in the first session [ 𝑉𝑖𝑠𝑢𝑎𝑙𝑢𝑠𝑒𝑟] because\n",
      "there was only one circle: the one following my hand. \" ).\n",
      "(3)Reduce the chance of disturbing user’s adjacent tasks compared\n",
      "topin-centered stop zones (six participants e.g., P8: \"In the\n",
      "first session [ 𝑉𝑖𝑠𝑢𝑎𝑙𝑝𝑖𝑛], the circles overlapped the task sometimes\n",
      "which was annoying. \" ). Indeed, projecting zones over the surface\n",
      "of multiple actuated devices (i.e., pins) at the same time can\n",
      "spread over the surfaces of neighboring actuated devices (e.g.,\n",
      "other pins) and hence potentially disturb the user.\n",
      "TEI ’25, March 04–07, 2025, Bordeaux / Talence, France M. Daniel and W. Delamare\n",
      "Figure 8: Results of interruption typ3es ( 𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡 ) and stop zone visualisations ( 𝑉𝑖𝑠𝑢𝑎𝑙 ) on perceived safety items (a), user\n",
      "experience items (b), and physiological signals (c).\n",
      "6.7.2 Users approaching pins in motion is perceived less safe\n",
      "than pins approaching users. When approaching pins in motion,\n",
      "users felt more stress and anxiety than having pins in motion\n",
      "approaching them. The intention of the shape display was not\n",
      "clear to them and its behavior was hard to predict when users\n",
      "approached pins in motion (3 participants, e.g., P6: \"The first\n",
      "two sessions [ 𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑢𝑠𝑒𝑟] were more stressful than the last two\n",
      "[𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑠𝑦𝑠𝑡𝑒𝑚 ]. It is not easy to predict the robot behavior when\n",
      "it is already moving. We do not know how the system will react when\n",
      "we move towards it.\" ,P7: \"During these sessions [ 𝐼𝑛𝑡𝑒𝑟𝑟𝑢𝑝𝑡𝑢𝑠𝑒𝑟],\n",
      "I was more anxious because I needed to reach the target while\n",
      "the cylinders were still moving.\" ). The desire for explainability\n",
      "and predictability are essential for the perception of safety [ 15]:\n",
      "clear robot’s intentions contribute to the perceived safety [ 51].\n",
      "Previous works used visual cues to communicate the motion\n",
      "intent of robots [ 5,56,62] (e.g., by continuously visualizing a\n",
      "drone’s trajectory using AR headsets [ 62]). While such visual cues\n",
      "help users anticipate and predict robot’s directional motion, they\n",
      "partially or completely occlude the robot. However, the surface\n",
      "of SCIs is commonly used as an output modality for displaying\n",
      "application information via a GUI. Thus, there is a design challenge\n",
      "in communicating shape-change intents using the interface’s visual\n",
      "output modality potentially already use by the GUI.7 Main takeaways\n",
      "From the two user studies, we understand that:\n",
      "•Enforcing three speed zones with system-triggered pin motions\n",
      "rather than two speed zones improves the perceived safety.\n",
      "•Enforcing two speed zones orthree speed zones with user-triggered\n",
      "pin motions depends on user preferences.\n",
      "•Visualizing a single user-centered stop zone rather than multiple\n",
      "pin-centered stop zones improves the perceived safety.\n",
      "•Users approaching pins in motion is perceived as less safe than\n",
      "pins approaching users.\n",
      "8 Limitations and Perspectives\n",
      "8.1 A First Step into the Concept of Perceived\n",
      "Safety with SCIs\n",
      "We performed two controlled laboratory experiments to get insights\n",
      "on the perceived safety of an unfamiliar actuated interface. There\n",
      "are several factors to explore to get a truly comprehensive overview\n",
      "of safety concepts such as population- ,context- , and implementation-\n",
      "related factors.\n",
      "Population factors. Participants never experienced a shape-\n",
      "changing interface before ShyPins (novelty effect), and knew\n",
      "that the experimenter was always present in case of problems\n",
      "(experimental observer effect). Both effects are known to limit the\n",
      "true reactions of humans interacting with actuated devices [ 2]. In\n",
      "ShyPins TEI ’25, March 04–07, 2025, Bordeaux / Talence, France\n",
      "addition, we had a relatively small sample size of a highly-educated\n",
      "young adult population. Future work should also explore individual\n",
      "characteristics factors such as personality traits (e.g., neuroticism,\n",
      "pessimism, and optimism), gender, age, education, experience, or\n",
      "culture to evaluate the safety perception of SCIs [ 2,27]. Lastly,\n",
      "in-depth evaluations in the wild of shape-change interfaces are\n",
      "important to assess suitable contexts of use, the fit between tasks\n",
      "and interfaces, and the cultural appropriation of shape-change [ 4].\n",
      "With safeguards in place, evaluations in the wild can responsibly\n",
      "proceed, allowing users to freely interact with these actuated\n",
      "interfaces.\n",
      "Contextual factors. In both studies, participants used a single\n",
      "hand to physically interact with a pin-based surface, and always\n",
      "faced the pin matrix actuation. In addition, the pin-based surface\n",
      "was a small shape display with low matrix resolution and large pins.\n",
      "Yet, other contexts can lead to different results [ 46]. For instance,\n",
      "(1) the greater distance between the human and the actuated device,\n",
      "the safer the device is perceived, (2) some directions of approach\n",
      "(e.g., from the back rather than from the front) are perceived as\n",
      "less safe, and (3) actuated devices that are larger or with certain\n",
      "features (e.g., shape and color) are perceived as less safe, regardless\n",
      "of their motion [46].\n",
      "Implementation factors. A ShyPins pin can move at a maximal\n",
      "speed of 0.05m/s, while other comfortable speed thresholds are\n",
      "equal to 0.5m/s for industrial manipulators [ 6], and in the 0.5–0.7m/s\n",
      "range for drones [ 22,63], for example. Yet, dynamic physical\n",
      "interaction can benefit from faster shape-change. Thus, there is\n",
      "a challenge in identifying the trade-off between (i) faster shape-\n",
      "change like other large actuated devices (10 times faster than ours\n",
      "and other recent prototypes [ 18,20,21]), and (ii) the perceived\n",
      "safety. In addition, ShyPins’ safeguard reduces pin motion speed\n",
      "using a discrete number of separation zones (0% - 100% speed zones\n",
      "with SMS and 0% - 50% - 100% speed zones with SSM). Future work\n",
      "should explore how the discretization and the resulting velocity\n",
      "profiles influence the perceived safety of SCIs. For instance, a\n",
      "single continuous separation zone [ 29] might lead to smoother\n",
      "pin motion and thereby improve the perceived safety of pin-based\n",
      "shape displays.\n",
      "8.2 Safeguard and Prototyping Perspectives\n",
      "We describe two research perspectives regarding both the safeguard\n",
      "visualization and the safeguard behavior .\n",
      "Safeguard Visualization. The usability and the perceived safety\n",
      "of shape-changing surfaces can benefit from the 3D visualization\n",
      "of safety volumes using AR headsets [ 19,49]. Indeed, the human\n",
      "body and actuated pins moves in 3D, enforcing separation zones\n",
      "between them basically creates safety volumes. In the second user\n",
      "study, we used spatial augmented reality to project only the safety\n",
      "zones’ edges onto the shape-changing surface. While this approach\n",
      "displays only the necessary information to explain why a pin is\n",
      "paused and prevents users from wearing additional devices, it\n",
      "reduces safety volumes to safety lines.\n",
      "Besides the safety zones visualization, some participants felt\n",
      "stressed or anxious because the intention of the pin-based shape\n",
      "display was not clear to them: its behavior was hard to predictwhen they were approaching pins in motion. On could explore if\n",
      "extra-apparatus such as AR headset can help solve the intention\n",
      "problem.\n",
      "Safeguard behavior. Our first study shows that safety perception\n",
      "of shape-changing interfaces can be improved by adapting shape-\n",
      "change to user’s visual attention and emotions:\n",
      "•Periphery of attention or System-controlled actuation : slow pin\n",
      "motions are perceived safer than fast motions.\n",
      "•User-controlled actuation : speed of pin motion can be adapted to\n",
      "users’ profile.\n",
      "A promising perspective is to use machine and deep learning\n",
      "with SCIs to estimate and predict users’ mental states (e.g.,\n",
      "visual attention [ 31], fear [ 10]) from external measures (e.g., head\n",
      "orientation, eye movement, facial expression) and/or internal\n",
      "measures (e.g., brain activity, galvanic skin response, heart rate). For\n",
      "instance, a safeguard knowing user’s visual attention and emotions\n",
      "could improve the perceived safety of a pin-based shape display\n",
      "by automatically enforcing a greater speed limitation (1) on a fast\n",
      "system-controlled pin motion happening in user’s periphery of\n",
      "attention, or (2) on a user-controlled pin motion in the focus of\n",
      "attention that previously triggered a negative response from users.\n",
      "8.3 Implications for the SCI community\n",
      "As noted by Alexander et al. [ 4], passive acceptance of safety\n",
      "standards from the robotics community may result in guidelines\n",
      "that do not align with the goals of shape-changing interfaces,\n",
      "thereby limiting the dynamic physical interaction these systems\n",
      "aim to provide. Therefore, the SCI community must address the\n",
      "following questions:\n",
      "(1)What makes a shape-change mentally unsafe? This\n",
      "question requires understanding the factors that contribute to\n",
      "mental discomfort during dynamic physical interaction, which\n",
      "recent works in robotics have comprehensively reviewed [ 2,\n",
      "46]. However, it is essential to investigate whether mental\n",
      "discomfort is universally experienced across all types of shape-\n",
      "changing interfaces. Indeed, while this work highlights that\n",
      "perceived safety is in fact a relevant question with pin-based\n",
      "shape displays, the SCI community has created various interface\n",
      "types such as line-based [ 35], string-based [ 16], and swarm-\n",
      "based [ 30] to name a few. For example, larger line-based\n",
      "interfaces have been observed to startle users when quickly\n",
      "changing form [ 35]. In addition, we also should account for\n",
      "various tasks (e.g., rendering tangible controls, simulating\n",
      "material properties), environments (e.g., at home, at work), and\n",
      "users (e.g., novice, expert). Understanding these variables will\n",
      "help in developing tailored safety standards that accommodate\n",
      "the capabilities of shape-changing interfaces.\n",
      "(2)How can we predict when a requested shape-change is\n",
      "going to be mentally unsafe? Predicting mental unsafety\n",
      "involves identifying specific triggers or scenarios that may lead\n",
      "to mental discomfort. This may include sudden or unexpected\n",
      "changes in shape (slow versus fast pin motion around user),\n",
      "perceived loss of control (system versus user-triggered pin\n",
      "motion), or interaction (unforced versus forced pin motion in\n",
      "contact with the user) that evoke anxiety or stress. Developing\n",
      "self-learning agents, predictive models or heuristics based on\n",
      "TEI ’25, March 04–07, 2025, Bordeaux / Talence, France M. Daniel and W. Delamare\n",
      "empirical data and user feedback will be crucial in foreseeing\n",
      "potentially unsafe interaction before they occur (e.g., predicting\n",
      "that the requested pin-based tangible control rendering will be\n",
      "too fast or too close for user’s preferences) [66].\n",
      "(3)How can we mitigate a requested shape-change predicted\n",
      "as mentally unsafe while minimizing the impact on\n",
      "dynamic physical interaction? Mitigation strategies must\n",
      "balance ensuring mental safety with preserving the dynamic\n",
      "capabilities of SCIs. In this work, we chose to explore standard\n",
      "collision avoidance strategies alone (i.e., slowing and stopping\n",
      "pin motions). However, there are other complementary options.\n",
      "For instance, path planning [ 26,34] could be explored to handle\n",
      "shape-changes (e.g., z-axis translation of actuated pins, xy-axis\n",
      "bending of actuated lines [ 35], xy-axis translation of actuated\n",
      "dots [ 37]) predicted as mentally unsafe by (i) avoiding the user\n",
      "if possible, and (ii) stopping if avoidance fails. In addition to\n",
      "contribute to the safety topic, such options will also imply\n",
      "advancements in SCI prototype capabilities (e.g., enabling\n",
      "active xy-axis bending and translation of individual pins to\n",
      "deviate their trajectory), representing significant technical\n",
      "contributions.\n",
      "9 Conclusion\n",
      "In this paper, we highlight that a pin-based shape display, lacking\n",
      "proper safeguards, can forcefully push pins into a user’s arm or\n",
      "pull pins from the user’s hand, making tangible controls appear or\n",
      "disappear unexpectedly. To address this issue, we introduce ShyPins\n",
      "– a 5x6 pin-based shape display with a safeguard that regulates\n",
      "pin motion based on proximity to the user’s body by enforcing\n",
      "speed safety zones. In a first lab study, we compared user safety\n",
      "perceptions of a pin-based shape display using two or three speed\n",
      "zones. We found that for system-triggered pin actuation, three speed\n",
      "zones are preferred and perceived as safer than two. However, for\n",
      "user-triggered pin actuation, preferences vary based on individual\n",
      "characteristics: some users favoring two zones for quick motions\n",
      "and others preferring three for calmer motions. Pausing pin motions\n",
      "can create an discrepancy between the pins’ state and the digital\n",
      "data they should represent (e.g., when a physical bar chart partially\n",
      "rendered due to paused pins). Thus, in a second lab study, we\n",
      "compared user safety perceptions of a pin-based shape display\n",
      "projecting human-centered orpin-centered stop zones onto a pin-\n",
      "based surface to inform users about the discrepancy i.e., paused\n",
      "pins. We found that a single human-centered stop zone is preferred\n",
      "and perceived as safer than multiple pin-centered stop zones: it\n",
      "improves users’ feeling of safety and control, and reduces users’\n",
      "mental workload and visual distraction. Additionally, users perceive\n",
      "pin-based shape displays as less safe when approaching moving\n",
      "pins (unpredictable intent) compared to when pins approach body\n",
      "parts.\n",
      "Acknowledgments\n",
      "This Project is financed by the Interreg Atlantic Area Program\n",
      "through the European Regional Development Fund (ERDF) from\n",
      "the European Commission.References\n",
      "[1]Neziha Akalin, Annica Kristoffersson, and Amy Loutfi. 2019. Evaluating the Sense\n",
      "of Safety and Security in Human–Robot Interaction with Older People. In Social\n",
      "Robots: Technological, Societal and Ethical Aspects of Human-Robot Interaction ,\n",
      "Oliver Korn (Ed.). Springer International Publishing, Cham, 237–264. https://doi.\n",
      "org/10.1007/978-3-030-17107-0_12 Series Title: Human–Computer Interaction\n",
      "Series.\n",
      "[2]Neziha Akalin, Annica Kristoffersson, and Amy Loutfi. 2022. Do you feel safe\n",
      "with your robot? Factors influencing perceived safety in human-robot interaction\n",
      "based on subjective and objective measures. International Journal of Human-\n",
      "Computer Studies 158 (Feb. 2022), 102744. https://doi.org/10.1016/j.ijhcs.2021.\n",
      "102744\n",
      "[3]R. Alami, R. Chatila, S. Fleury, M. Ghallab, and F. Ingrand. 1998. An Architecture\n",
      "for Autonomy. The International Journal of Robotics Research 17, 4 (April 1998),\n",
      "315–337. https://doi.org/10.1177/027836499801700402\n",
      "[4]Jason Alexander, Anne Roudaut, Jürgen Steimle, Kasper Hornbæk, Miguel\n",
      "Bruns Alonso, Sean Follmer, and Timothy Merritt. 2018. Grand Challenges\n",
      "in Shape-Changing Interface Research. In Proceedings of the 2018 CHI Conference\n",
      "on Human Factors in Computing Systems (CHI ’18) . Association for Computing\n",
      "Machinery, Montreal QC, Canada, 1–14. https://doi.org/10.1145/3173574.3173873\n",
      "[5]Rasmus S. Andersen, Ole Madsen, Thomas B. Moeslund, and Heni Ben Amor.\n",
      "2016. Projecting robot intentions into human environments. In 2016 25th IEEE\n",
      "International Symposium on Robot and Human Interactive Communication (RO-\n",
      "MAN) . IEEE, New York, NY, USA, 294–301. https://doi.org/10.1109/ROMAN.\n",
      "2016.7745145\n",
      "[6]T. Arai, R. Kato, and M. Fujita. 2010. Assessment of operator stress induced by\n",
      "robot collaboration in assembly. CIRP Annals 59, 1 (2010), 5–8. https://doi.org/\n",
      "10.1016/j.cirp.2010.03.043\n",
      "[7]Christoph Bartneck, Dana Kulić, Elizabeth Croft, and Susana Zoghbi. 2009.\n",
      "Measurement Instruments for the Anthropomorphism, Animacy, Likeability,\n",
      "Perceived Intelligence, and Perceived Safety of Robots. International Journal of\n",
      "Social Robotics 1, 1 (Jan. 2009), 71–81. https://doi.org/10.1007/s12369-008-0001-3\n",
      "[8]Christoph H. Belke and Jamie Paik. 2017. Mori: A Modular Origami Robot.\n",
      "IEEE/ASME Transactions on Mechatronics 22, 5 (Oct. 2017), 2153–2164. https:\n",
      "//doi.org/10.1109/TMECH.2017.2697310\n",
      "[9]Christopher Brandl, Alexander Mertens, and Christopher M. Schlick. 2016.\n",
      "Human-Robot Interaction in Assisted Personal Services: Factors Influencing\n",
      "Distances That Humans Will Accept between Themselves and an Approaching\n",
      "Service Robot: Human-Robot Interaction in Assisted Personal Services. Human\n",
      "Factors and Ergonomics in Manufacturing & Service Industries 26, 6 (Nov. 2016),\n",
      "713–727. https://doi.org/10.1002/hfm.20675\n",
      "[10] Oana Bălan, Gabriela Moise, Alin Moldoveanu, Marius Leordeanu, and Florica\n",
      "Moldoveanu. 2020. An Investigation of Various Machine and Deep Learning\n",
      "Techniques Applied in Automatic Fear Level Detection and Acrophobia Virtual\n",
      "Therapy. Sensors 20, 2 (Jan. 2020), 496. https://doi.org/10.3390/s20020496\n",
      "[11] Tiffany L. Chen, Chih-Hung Aaron King, Andrea L. Thomaz, and Charles C.\n",
      "Kemp. 2014. An Investigation of Responses to Robot-Initiated Touch in a Nursing\n",
      "Context. International Journal of Social Robotics 6, 1 (Jan. 2014), 141–161. https:\n",
      "//doi.org/10.1007/s12369-013-0215-x\n",
      "[12] Maxime Daniel, Guillaume Rivière, and Nadine Couture. 2019. CairnFORM:\n",
      "a Shape-Changing Ring Chart Notifying Renewable Energy Availability in\n",
      "Peripheral Locations. In Proceedings of the Thirteenth International Conference\n",
      "on Tangible, Embedded, and Embodied Interaction - TEI ’19 . ACM Press, Tempe,\n",
      "Arizona, USA, 275–286. https://doi.org/10.1145/3294109.3295634\n",
      "[13] Francesco De Pace, Federico Manuri, Andrea Sanna, and Claudio Fornaro. 2020.\n",
      "A systematic review of Augmented Reality interfaces for collaborative industrial\n",
      "robots. Computers & Industrial Engineering 149 (Nov. 2020), 106806. https:\n",
      "//doi.org/10.1016/j.cie.2020.106806\n",
      "[14] Debargha Dey, Marieke Martens, Berry Eggen, and Jacques Terken. 2019.\n",
      "Pedestrian road-crossing willingness as a function of vehicle automation, external\n",
      "appearance, and driving behaviour. Transportation Research Part F: Traffic\n",
      "Psychology and Behaviour 65 (Aug. 2019), 191–205. https://doi.org/10.1016/\n",
      "j.trf.2019.07.027\n",
      "[15] Eric Eller and Dieter Frey. 2019. Psychological Perspectives on Perceived Safety:\n",
      "Social Factors of Feeling Safe. In Perceived Safety , Martina Raue, Bernhard\n",
      "Streicher, and Eva Lermer (Eds.). Springer International Publishing, Cham, 43–60.\n",
      "https://doi.org/10.1007/978-3-030-11456-5_4 Series Title: Risk Engineering.\n",
      "[16] Severin Engert, Konstantin Klamka, Andreas Peetz, and Raimund Dachselt. 2022.\n",
      "STRAIDE: A Research Platform for Shape-Changing Spatial Displays based on\n",
      "Actuated Strings. In CHI Conference on Human Factors in Computing Systems .\n",
      "ACM, New Orleans LA USA, 1–16. https://doi.org/10.1145/3491102.3517462\n",
      "[17] Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge, and Hiroshi Ishii.\n",
      "2013. inFORM: Dynamic Physical Affordances and Constraints Through Shape\n",
      "and Object Actuation. In Proceedings of the 26th Annual ACM Symposium on\n",
      "User Interface Software and Technology (UIST ’13) . ACM, New York, NY, USA,\n",
      "417–426. https://doi.org/10.1145/2501988.2502032 event-place: St. Andrews,\n",
      "Scotland, United Kingdom.\n",
      "ShyPins TEI ’25, March 04–07, 2025, Bordeaux / Talence, France\n",
      "[18] Eric J Gonzalez, Eyal Ofek, Mar Gonzalez-Franco, and Mike Sinclair. 2021. X-\n",
      "Rings: A Hand-mounted 360 °Shape Display for Grasping in Virtual Reality. In\n",
      "The 34th Annual ACM Symposium on User Interface Software and Technology .\n",
      "ACM, Virtual Event USA, 732–742. https://doi.org/10.1145/3472749.3474782\n",
      "[19] Uwe Gruenefeld, Lars Prädel, Jannike Illing, Tim Stratmann, Sandra Drolshagen,\n",
      "and Max Pfingsthorn. 2020. Mind the ARm: realtime visualization of robot motion\n",
      "intent in head-mounted augmented reality. In Proceedings of the Conference on\n",
      "Mensch und Computer . ACM, Magdeburg Germany, 259–266. https://doi.org/10.\n",
      "1145/3404983.3405509\n",
      "[20] John Hardy, Christian Weichel, Faisal Taher, John Vidler, and Jason Alexander.\n",
      "2015. ShapeClip: Towards Rapid Prototyping with Shape-Changing Displays for\n",
      "Designers. In Proceedings of the 33rd Annual ACM Conference on Human Factors\n",
      "in Computing Systems - CHI ’15 . ACM Press, Seoul, Republic of Korea, 19–28.\n",
      "https://doi.org/10.1145/2702123.2702599\n",
      "[21] Seungwoo Je, Hyunseung Lim, Kongpyung Moon, Shan-Yuan Teng, Jas Brooks,\n",
      "Pedro Lopes, and Andrea Bianchi. 2021. Elevate: A Walkable Pin-Array for\n",
      "Large Shape-Changing Terrains. In Proceedings of the 2021 CHI Conference on\n",
      "Human Factors in Computing Systems . ACM, Yokohama Japan, 1–11. https:\n",
      "//doi.org/10.1145/3411764.3445454\n",
      "[22] Walther Jensen, Simon Hansen, and Hendrik Knoche. 2018. Knowing You,\n",
      "Seeing Me: Investigating User Preferences in Drone-Human Acknowledgement.\n",
      "InProceedings of the 2018 CHI Conference on Human Factors in Computing Systems .\n",
      "ACM, Montreal QC Canada, 1–12. https://doi.org/10.1145/3173574.3173939\n",
      "[23] Waldemar Karwowski and Mansour Rahimi. 1991. Worker selection of safe speed\n",
      "and idle condition in simulated monitoring of two industrial robots. Ergonomics\n",
      "34, 5 (May 1991), 531–546. https://doi.org/10.1080/00140139108967335\n",
      "[24] Yuichiro Katsumoto. 2020. Inside Out. In ACM SIGGRAPH 2020 Art Gallery\n",
      "(SIGGRAPH ’20) . Association for Computing Machinery, New York, NY, USA, 467.\n",
      "https://doi.org/10.1145/3386567.3388556 event-place: Virtual Event, USA.\n",
      "[25] Hyunyoung Kim. 2018. Fostering Design Process of Shape-Changing Interfaces.\n",
      "InThe 31st Annual ACM Symposium on User Interface Software and Technology\n",
      "Adjunct Proceedings - UIST ’18 Adjunct . ACM Press, Berlin, Germany, 224–227.\n",
      "https://doi.org/10.1145/3266037.3266131\n",
      "[26] Dana Kulić and Elizabeth Croft. 2007. Pre-collision safety strategies for human-\n",
      "robot interaction. Autonomous Robots 22 (2007), 149–164.\n",
      "[27] Przemyslaw A. Lasota, Terrence Fong, and Julie A. Shah. 2017. A Survey of\n",
      "Methods for Safe Human-Robot Interaction. Foundations and Trends ®in Robotics\n",
      "5, 4 (2017), 261–349. https://doi.org/10.1561/2300000052\n",
      "[28] Przemyslaw A. Lasota, Gregory F. Rossano, and Julie A. Shah. 2014. Toward\n",
      "safe close-proximity human-robot interaction with standard industrial robots. In\n",
      "2014 IEEE International Conference on Automation Science and Engineering (CASE) .\n",
      "IEEE, Taipei, 339–344. https://doi.org/10.1109/CoASE.2014.6899348\n",
      "[29] Przemyslaw A. Lasota and Julie A. Shah. 2015. Analyzing the Effects of Human-\n",
      "Aware Motion Planning on Close-Proximity Human–Robot Collaboration.\n",
      "Human Factors: The Journal of the Human Factors and Ergonomics Society 57,\n",
      "1 (Feb. 2015), 21–33. https://doi.org/10.1177/0018720814565188\n",
      "[30] Mathieu Le Goc, Lawrence H. Kim, Ali Parsaei, Jean-Daniel Fekete, Pierre\n",
      "Dragicevic, and Sean Follmer. 2016. Zooids: Building Blocks for Swarm User\n",
      "Interfaces. In Proceedings of the 29th Annual Symposium on User Interface Software\n",
      "and Technology - UIST ’16 . ACM Press, Tokyo, Japan, 97–109. https://doi.org/10.\n",
      "1145/2984511.2984547\n",
      "[31] Xiangdong Li, Yifei Shan, Wenqian Chen, Yue Wu, Praben Hansen, and Simon\n",
      "Perrault. 2021. Predicting user visual attention in virtual reality with a deep\n",
      "learning model. Virtual Reality 25, 4 (Dec. 2021), 1123–1136. https://doi.org/10.\n",
      "1007/s10055-021-00512-7\n",
      "[32] Andrés Lucero. 2015. Using Affinity Diagrams to Evaluate Interactive Prototypes.\n",
      "InHuman-Computer Interaction – INTERACT 2015 , Julio Abascal, Simone Barbosa,\n",
      "Mirko Fetter, Tom Gross, Philippe Palanque, and Marco Winckler (Eds.). Vol. 9297.\n",
      "Springer International Publishing, Cham, 231–248. https://doi.org/10.1007/978-\n",
      "3-319-22668-2_19 Series Title: Lecture Notes in Computer Science.\n",
      "[33] Dominique Makowski, Tam Pham, Zen J. Lau, Jan C. Brammer, François\n",
      "Lespinasse, Hung Pham, Christopher Schölzel, and S. H. Annabel Chen. 2021.\n",
      "NeuroKit2: A Python toolbox for neurophysiological signal processing. Behavior\n",
      "Research Methods 53, 4 (Aug. 2021), 1689–1696. https://doi.org/10.3758/s13428-\n",
      "020-01516-y\n",
      "[34] Debasmita Mukherjee, Kashish Gupta, Li Hsin Chang, and Homayoun Najjaran.\n",
      "2022. A survey of robot learning strategies for human-robot collaboration in\n",
      "industrial settings. Robotics and Computer-Integrated Manufacturing 73 (2022),\n",
      "102231.\n",
      "[35] Ken Nakagaki, Sean Follmer, and Hiroshi Ishii. 2015. LineFORM: Actuated Curve\n",
      "Interfaces for Display, Interaction, and Constraint. In Proceedings of the 28th\n",
      "Annual ACM Symposium on User Interface Software & Technology . ACM, Charlotte\n",
      "NC USA, 333–339. https://doi.org/10.1145/2807442.2807452\n",
      "[36] Ken Nakagaki, Yingda (Roger) Liu, Chloe Nelson-Arzuaga, and Hiroshi Ishii.\n",
      "2020. TRANS-DOCK: Expanding the Interactivity of Pin-based Shape Displays by\n",
      "Docking Mechanical Transducers. In Proceedings of the Fourteenth International\n",
      "Conference on Tangible, Embedded, and Embodied Interaction . ACM, Sydney NSW\n",
      "Australia, 131–142. https://doi.org/10.1145/3374920.3374933[37] Ken Nakagaki, Jordan L Tappa, Yi Zheng, Jack Forman, Joanne Leong, Sven\n",
      "Koenig, and Hiroshi Ishii. 2022. (Dis)Appearables: A Concept and Method\n",
      "for Actuated Tangible UIs to Appear and Disappear based on Stages. In CHI\n",
      "Conference on Human Factors in Computing Systems . ACM, New Orleans LA USA,\n",
      "1–13. https://doi.org/10.1145/3491102.3501906\n",
      "[38] Akira Nakayasu. 2016. Luminescent Tentacles: A Scalable SMA Motion Display. In\n",
      "Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software\n",
      "and Technology (Tokyo, Japan) (UIST ’16 Adjunct) . Association for Computing\n",
      "Machinery, New York, NY, USA, 33–34. https://doi.org/10.1145/2984751.2985695\n",
      "[39] Yu Ogura, Masakazu Fujii, Kazuyuki Nishijima, Hiroki Murakami, Mitsuharu\n",
      "Sonehara, and IHI Corporation, 1 Shin-Nakahara-cho, Isogo-ku, Yokohama-\n",
      "shi, Kanagawa 235-8501, Japan. 2012. Applicability of Hand-Guided Robot for\n",
      "Assembly-Line Work. Journal of Robotics and Mechatronics 24, 3 (June 2012),\n",
      "547–552. https://doi.org/10.20965/jrm.2012.p0547\n",
      "[40] Jennifer Pearson, Simon Robinson, Matt Jones, and Céline Coutrix. 2017.\n",
      "Evaluating deformable devices with emergent users. In Proceedings of the 19th\n",
      "International Conference on Human-Computer Interaction with Mobile Devices and\n",
      "Services . ACM, Vienna Austria, 1–7. https://doi.org/10.1145/3098279.3098555\n",
      "[41] Laura Pruszko, Céline Coutrix, Yann Laurillau, Benoît Piranda, and Julien\n",
      "Bourgeois. 2021. Molecular HCI: Structuring the Cross-disciplinary Space of\n",
      "Modular Shape-changing User Interfaces. Proceedings of the ACM on Human-\n",
      "Computer Interaction 5, EICS (May 2021), 1–33. https://doi.org/10.1145/3461733\n",
      "[42] Mansour Rahimi and Waldemar Karwowski. 1990. Human perception of robot\n",
      "safe speed and idle time. Behaviour & Information Technology 9, 5 (Sept. 1990),\n",
      "381–389. https://doi.org/10.1080/01449299008924252\n",
      "[43] Majken Kirkegård Rasmussen and Fabian Hemmert. 2019. Envisioning Future\n",
      "Challenges and Possibilities for Shape-Changing Interfaces through Speculative\n",
      "Scenarios. In Proceedings of Mensch und Computer 2019 (MuC’19) . Association for\n",
      "Computing Machinery, New York, NY, USA, 487–492. https://doi.org/10.1145/\n",
      "3340764.3344444\n",
      "[44] Majken Kirkegård Rasmussen, Timothy Merritt, Miguel Bruns Alonso, and\n",
      "Marianne Graves Petersen. 2016. Balancing User and System Control in Shape-\n",
      "Changing Interfaces: a Designerly Exploration. In Proceedings of the TEI ’16:\n",
      "Tenth International Conference on Tangible, Embedded, and Embodied Interaction -\n",
      "TEI ’16 . ACM Press, Eindhoven, Netherlands, 202–210. https://doi.org/10.1145/\n",
      "2839462.2839499\n",
      "[45] Simon Robinson, Céline Coutrix, Jennifer Pearson, Juan Rosso, Matheus Fernan-\n",
      "des Torquato, Laurence Nigay, and Matt Jones. 2016. Emergeables: Deformable\n",
      "Displays for Continuous Eyes-Free Mobile Interaction. In Proceedings of the 2016\n",
      "CHI Conference on Human Factors in Computing Systems . ACM, San Jose California\n",
      "USA, 3793–3805. https://doi.org/10.1145/2858036.2858097\n",
      "[46] Matteo Rubagotti, Inara Tusseyeva, Sara Baltabayeva, Danna Summers, and Anara\n",
      "Sandygulova. 2022. Perceived safety in physical human–robot interaction—A\n",
      "survey. Robotics and Autonomous Systems 151 (May 2022), 104047. https://doi.\n",
      "org/10.1016/j.robot.2022.104047\n",
      "[47] Maha Salem, Gabriella Lakatos, Farshid Amirabdollahian, and Kerstin Dauten-\n",
      "hahn. 2015. Towards Safe and Trustworthy Social Robots: Ethical Challenges\n",
      "and Practical Issues. In Social Robotics , Adriana Tapus, Elisabeth André, Jean-\n",
      "Claude Martin, François Ferland, and Mehdi Ammi (Eds.). Vol. 9388. Springer\n",
      "International Publishing, Cham, 584–593. https://doi.org/10.1007/978-3-319-\n",
      "25554-5_58 Series Title: Lecture Notes in Computer Science.\n",
      "[48] Pericle Salvini, Diego Paez-Granados, and Aude Billard. 2021. On the Safety of\n",
      "Mobile Robots Serving in Public Spaces: Identifying gaps in EN ISO 13482:2014\n",
      "and calling for a new standard. ACM Transactions on Human-Robot Interaction\n",
      "10, 3 (Sept. 2021), 1–27. https://doi.org/10.1145/3442678\n",
      "[49] Ane San Martín and Johan Kildal. 2019. Audio-visual AR to Improve Awareness\n",
      "of Hazard Zones Around Robots. In Extended Abstracts of the 2019 CHI Conference\n",
      "on Human Factors in Computing Systems . ACM, Glasgow Scotland Uk, 1–6. https:\n",
      "//doi.org/10.1145/3290607.3312996\n",
      "[50] Martin Schrepp, Andreas Hinderks, and Jörg Thomaschewski. 2017. Design and\n",
      "Evaluation of a Short Version of the User Experience Questionnaire (UEQ-S).\n",
      "International Journal of Interactive Multimedia and Artificial Intelligence 4, 6 (2017),\n",
      "103. https://doi.org/10.9781/ijimai.2017.09.001\n",
      "[51] Emrah Akin Sisbot, Luis F. Marin-Urias, Xavier Broquère, Daniel Sidobre, and\n",
      "Rachid Alami. 2010. Synthesizing Robot Motions Adapted to Human Presence:\n",
      "A Planning and Control Framework for Safe and Socially Acceptable Robot\n",
      "Motions. International Journal of Social Robotics 2, 3 (Sept. 2010), 329–343. https:\n",
      "//doi.org/10.1007/s12369-010-0059-6\n",
      "[52] Alexa F. Siu, Eric J. Gonzalez, Shenli Yuan, Jason Ginsberg, Allen Zhao, and Sean\n",
      "Follmer. 2017. shapeShift: A Mobile Tabletop Shape Display for Tangible and\n",
      "Haptic Interaction. In Adjunct Publication of the 30th Annual ACM Symposium on\n",
      "User Interface Software and Technology - UIST ’17 . ACM Press, Qu&#233;bec City,\n",
      "QC, Canada, 77–79. https://doi.org/10.1145/3131785.3131792\n",
      "[53] Miriam Sturdee and Jason Alexander. 2018. Analysis and Classification of Shape-\n",
      "Changing Interfaces for Design and Application-based Research. Comput. Surveys\n",
      "51, 1 (Jan. 2018), 2:1–2:32. https://doi.org/10.1145/3143559\n",
      "[54] Ryo Suzuki, Junichi Yamaoka, Daniel Leithinger, Tom Yeh, Mark D. Gross,\n",
      "Yoshihiro Kawahara, and Yasuaki Kakehi. 2018. Dynablock: Dynamic 3D Printing\n",
      "TEI ’25, March 04–07, 2025, Bordeaux / Talence, France M. Daniel and W. Delamare\n",
      "for Instant and Reconstructable Shape Formation. In Proceedings of the 31st\n",
      "Annual ACM Symposium on User Interface Software and Technology . ACM, Berlin\n",
      "Germany, 99–111. https://doi.org/10.1145/3242587.3242659\n",
      "[55] Ryo Suzuki, Clement Zheng, Yasuaki Kakehi, Tom Yeh, Ellen Yi-Luen Do, Mark D.\n",
      "Gross, and Daniel Leithinger. 2019. ShapeBots: Shape-changing Swarm Robots.\n",
      "InProceedings of the 32nd Annual ACM Symposium on User Interface Software\n",
      "and Technology (New Orleans, LA, USA) (UIST ’19) . Association for Computing\n",
      "Machinery, New York, NY, USA, 493–505. https://doi.org/10.1145/3332165.\n",
      "3347911\n",
      "[56] Daniel Szafir, Bilge Mutlu, and Terry Fong. 2015. Communicating Directionality\n",
      "in Flying Robots. In Proceedings of the Tenth Annual ACM/IEEE International\n",
      "Conference on Human-Robot Interaction . ACM, Portland Oregon USA, 19–26.\n",
      "https://doi.org/10.1145/2696454.2696475\n",
      "[57] Faisal Taher, Yvonne Jansen, Jonathan Woodruff, John Hardy, Kasper Hornbaek,\n",
      "and Jason Alexander. 2017. Investigating the Use of a Dynamic Physical Bar\n",
      "Chart for Data Exploration and Presentation. IEEE Transactions on Visualization\n",
      "and Computer Graphics 23, 1 (Jan. 2017), 451–460. https://doi.org/10.1109/TVCG.\n",
      "2016.2598498\n",
      "[58] Valeria Villani, Fabio Pini, Francesco Leali, and Cristian Secchi. 2018. Survey on\n",
      "human–robot collaboration in industrial settings: Safety, intuitive interfaces and\n",
      "applications. Mechatronics 55 (Nov. 2018), 248–266. https://doi.org/10.1016/j.\n",
      "mechatronics.2018.02.009\n",
      "[59] Christian Vogel, Markus Fritzsche, and Norbert Elkmann. 2016. Safe human-\n",
      "robot cooperation with high-payload robots in industrial applications. In 2016\n",
      "11th ACM/IEEE International Conference on Human-Robot Interaction (HRI) . IEEE,\n",
      "Christchurch, New Zealand, 529–530. https://doi.org/10.1109/HRI.2016.7451840[60] Christian Vogel, Christoph Walter, and Norbert Elkmann. 2017. Safeguarding\n",
      "and Supporting Future Human-robot Cooperative Manufacturing Processes by a\n",
      "Projection- and Camera-based Technology. Procedia Manufacturing 11 (2017),\n",
      "39–46. https://doi.org/10.1016/j.promfg.2017.07.127\n",
      "[61] Aleš Vysocký, Stefan Grushko, Petr Oščádal, Tomáš Kot, Ján Babjak, Rudolf Jánoš,\n",
      "Marek Sukop, and Zdenko Bobovský. 2020. Analysis of Precision and Stability\n",
      "of Hand Tracking with Leap Motion Sensor. Sensors 20, 15 (July 2020), 4088.\n",
      "https://doi.org/10.3390/s20154088\n",
      "[62] Michael Walker, Hooman Hedayati, Jennifer Lee, and Daniel Szafir. 2018.\n",
      "Communicating Robot Motion Intent with Augmented Reality. In Proceedings of\n",
      "the 2018 ACM/IEEE International Conference on Human-Robot Interaction . ACM,\n",
      "Chicago IL USA, 316–324. https://doi.org/10.1145/3171221.3171253\n",
      "[63] Anna Wojciechowska, Jeremy Frey, Sarit Sass, Roy Shafir, and Jessica R. Cauchard.\n",
      "2019. Collocated Human-Drone Interaction: Methodology and Approach Strategy.\n",
      "In2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI) .\n",
      "IEEE, Daegu, Korea (South), 172–181. https://doi.org/10.1109/HRI.2019.8673127\n",
      "[64] Alexander Yeh, Photchara Ratsamee, Kiyoshi Kiyokawa, Yuki Uranishi, Tomohiro\n",
      "Mashita, Haruo Takemura, Morten Fjeld, and Mohammad Obaid. 2017. Exploring\n",
      "Proxemics for Human-Drone Interaction. In Proceedings of the 5th International\n",
      "Conference on Human Agent Interaction . ACM, Bielefeld Germany, 81–88. https:\n",
      "//doi.org/10.1145/3125739.3125773\n",
      "[65] S Yim, C Sung, S Miyashita, D Rus, and S Kim. 2018. Animatronic soft robots by\n",
      "additive folding. The International Journal of Robotics Research 37, 6 (May 2018),\n",
      "611–628. https://doi.org/10.1177/0278364918772023\n",
      "[66] Angeliki Zacharaki, Ioannis Kostavelis, Antonios Gasteratos, and Ioannis Dokas.\n",
      "2020. Safety bounds in human robot interaction: A survey. Safety Science 127\n",
      "(July 2020), 104667. https://doi.org/10.1016/j.ssci.2020.104667\n",
      "</paper>\n",
      "\n",
      "Focus on methodology, findings, and clinical implications.\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import re\n",
    "\n",
    "# Helper function to clean up AI message content by removing <think> tags\n",
    "def sanitize(message: AIMessage) -> str:\n",
    "    \"\"\"Remove <think> tags and extra whitespace from the message content.\"\"\"\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", message.content, flags=re.DOTALL).strip()\n",
    "\n",
    "# Set up chatbot instructions and initialize\n",
    "chatbot = Chatbot(\n",
    "    llm=llm,\n",
    "    history=[SystemMessage(content=\"You are a helpful assistant. Answer my questions in a helpful way.\")]\n",
    ")\n",
    "\n",
    "# --- PROMPT 0: Summarize a medical research paper ---\n",
    "RESEARCH_PAPER = \"\\n\".join([\n",
    "    page.page_content for page in PyPDFLoader(\"files/daniel2025shypins.pdf\").lazy_load()\n",
    "])\n",
    "prompt0 = f\"\"\"Summarize this research paper.\\n\\n<paper>\\n{RESEARCH_PAPER}\\n</paper>\\n\\nFocus on motivation, methodology, and findings.\\n\"\"\"\n",
    "chatbot.clear_history()\n",
    "SUMMARY = sanitize(chatbot.invoke(prompt0))\n",
    "\n",
    "# --- PROMPT 1: Provide feedback on the summary ---\n",
    "prompt1 = f\"\"\"Your task is to provide feedback on a research paper summary. Here is a summary of a research paper:\\n\\n<summary>\\n{SUMMARY}\\n</summary>\\n\\nHere is the research paper:\\n\\n<paper>\\n{RESEARCH_PAPER}\\n</paper>\\n\\nReview this summary for accuracy, clarity, and completeness on a graded A-F scale.\\n\"\"\"\n",
    "chatbot.clear_history()\n",
    "FEEDBACK = sanitize(chatbot.invoke(prompt1))\n",
    "\n",
    "# --- PROMPT 2: Improve the summary based on feedback ---\n",
    "prompt2 = f\"\"\"Your task is to improve a paper summary given feedback. Here is the first draft of a medical research paper:\\n<summary>\\n{SUMMARY}\\n</summary>\\n\\nHere is the research paper:\\n<paper>\\n{RESEARCH_PAPER}\\n</paper>\\n\\nHere is the feedback:\\n<feedback>\\n{FEEDBACK}\\n</feedback>\\n\\nUpdate the summary based on the feedback.\\n\"\"\"\n",
    "chatbot.clear_history()\n",
    "CORRECTED_SUMMARY = sanitize(chatbot.invoke(prompt2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
